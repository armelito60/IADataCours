{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e039b0e",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Hands-on # 1</center>\n",
    "<center><span style=\"font-weight: bold; font-size: 1.8rem;\">Representing words and sentences</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967fd03",
   "metadata": {},
   "source": [
    "Since most of the algorithms existing out there are designed to handle numerical data, they are hardly applicable on raw texts. However, it is definitely possible to convert a text to a numerical representation.\n",
    "\n",
    "Ideal representations should handle **semantic**, **polysemy**, **irony** and lots of other specificities of texts. Along the decades, many text representations have been introduced to handle as many specificities as possible.\n",
    "\n",
    "In this hands-on, you will have to convert a given corpus of texts to various representations and highlight their pros / cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51c251",
   "metadata": {},
   "source": [
    "# Installation of required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b47516",
   "metadata": {},
   "source": [
    "The packages listed below should be installed. Using a virtual environment is highly recommended but not mandatory -- that is just good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efb6e6f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:44.509811Z",
     "start_time": "2021-10-25T13:01:44.504768Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint as pp\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "# Uncomment the following line to download the reuters dataset\n",
    "# nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eb3ae",
   "metadata": {},
   "source": [
    "**Note**: In NLP, we often add `<START>` and `<END>` tokens to represent the beginning and end of sentences, paragraphs or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a73a92",
   "metadata": {},
   "source": [
    "# Part 0 - Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777a255",
   "metadata": {},
   "source": [
    "The Reuters Corpus that we will use contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 categories.\n",
    "\n",
    "Before diving into word representations, let's explore it a little bit and simply preprocess its texts to make it more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac27760",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3f7a0",
   "metadata": {},
   "source": [
    "We will need to standardize all texts before converting anything to a numerical representations, since it will reduce the vocabulary size. Modify the following function to:\n",
    "\n",
    "* Add the `START_TOKEN` and `END_TOKEN` at the beginning and end of each document\n",
    "* Lowercase every words\n",
    "* Remove the punctuation from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f1c69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:58:47.303189Z",
     "start_time": "2021-10-25T12:58:47.288189Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(category=\"tea\", add_tokens=True):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "            add_token (boolean): whether to insert START_TOKEN or END_TOKEN in each document\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    \n",
    "    # Convert all words to lowercase and remove punctuation\n",
    "    corpus = [[w.lower().translate(str.maketrans('', '', string.punctuation)) for w in list(reuters.words(f))] for f in files]\n",
    "    corpus = [[word for word in doc if word and not word.isnumeric()] for doc in corpus]\n",
    "    # Add token if necessary\n",
    "    if add_tokens:\n",
    "        corpus = [[START_TOKEN] + doc + [END_TOKEN] for doc in corpus if doc]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9c36a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:58:48.144113Z",
     "start_time": "2021-10-25T12:58:47.351200Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = read_corpus(add_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946e85d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:58:48.206284Z",
     "start_time": "2021-10-25T12:58:48.191922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<START>', 'pakistan', 'confirms', 'kenya', 'tea', 'import', 'investigation',\n",
      "  'pakistan', 's', 'corporate', 'law', 'authority', 'cla', 'has', 'begun', 'an',\n",
      "  'enquiry', 'into', 'imports', 'of', 'tea', 'from', 'kenya', 'and', 'the',\n",
      "  'trade', 'imbalance', 'between', 'the', 'two', 'countries', 'cla', 'chairman',\n",
      "  'irtiza', 'husain', 'confirmed', 'he', 'told', 'reuters', 'by', 'telephone',\n",
      "  'that', 'importers', 'liptons', 'and', 'brooke', 'bond', 'had', 'been',\n",
      "  'asked', 'to', 'supply', 'data', 'to', 'the', 'authority', 'and', 'a',\n",
      "  'hearing', 'would', 'be', 'held', 'the', 'cla', 'would', 'then', 'report',\n",
      "  'back', 'to', 'the', 'commerce', 'ministry', 'which', 'had', 'requested',\n",
      "  'the', 'enquiry', 'husain', 'said', 'no', 'date', 'had', 'yet', 'been', 'set',\n",
      "  'for', 'the', 'hearing', 'and', 'declined', 'to', 'give', 'further',\n",
      "  'details', 'of', 'the', 'matter', 'industry', 'sources', 'told', 'reuters',\n",
      "  'reports', 'that', 'the', 'companies', 'tea', 'import', 'licences', 'had',\n",
      "  'been', 'suspended', 'were', 'incorrect', '<END>'],\n",
      " ['<START>', 'sri', 'lankan', 'tea', 'workers', 'launch', 'one', 'day',\n",
      "  'protest', 'thousands', 'of', 'tea', 'workers', 'of', 'indian', 'origin',\n",
      "  'went', 'on', 'strike', 'today', 'to', 'press', 'demands', 'for',\n",
      "  'citizenship', 'and', 'voting', 'rights', 'in', 'sri', 'lanka', 'a', 'union',\n",
      "  'statement', 'said', 'the', 'ceylon', 'workers', 'congress', 'cwc', 'said',\n",
      "  'its', 'members', 'launched', 'a', 'prayer', 'campaign', 'at', 'temples',\n",
      "  'and', 'other', 'places', 'in', 'a', 'non', 'violent', 'protest', 'to', 'get',\n",
      "  'the', 'authorities', 'to', 'expedite', 'citizenship', 'procedures', 'a',\n",
      "  'cwc', 'spokesman', 'said', 'a', 'three', 'day', 'campaign', 'was',\n",
      "  'suspended', 'after', 'a', 'cabinet', 'committee', 'promised', 'to', 'speed',\n",
      "  'up', 'procedures', 'under', 'a', 'new', 'set', 'of', 'regulations', 'trade',\n",
      "  'sources', 'said', 'the', 'strike', 'did', 'not', 'affect', 'production',\n",
      "  'or', 'today', 's', 'colombo', 'auction', '<END>']]\n"
     ]
    }
   ],
   "source": [
    "pp(corpus[:2], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009fb2f",
   "metadata": {},
   "source": [
    "# Part I - Word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9311f",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our texts we can represent them using vectors, also called embeddings in this case.\n",
    "\n",
    "*Note: the preprocessing done here is basic. We will see in another hands-on different preprocessing steps, including some suitable for frequentist approaches.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda63c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fee0b",
   "metadata": {},
   "source": [
    "Each word representation has its pros and cons: understanding them will help you in finding the best representation that suits your use case. \n",
    "\n",
    "As a result, you will have to implement / load and analyze the behaviour of word vectors coming from:\n",
    "\n",
    "* Dummy encoding\n",
    "* Co-occurence matrix encoding\n",
    "* Pretrained GloVe encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366603e",
   "metadata": {},
   "source": [
    "## Dummy encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bf1b1",
   "metadata": {},
   "source": [
    "The dummy encoding consist in encoding each individual word of our corpus into a vector filled with 0 expect at a specific position where the value is 1 (equivalent to the encoding of categorical variables).\n",
    "\n",
    "As discussed during the course, those embeddings are kind of pointless since they don't handle a single element of the ideal word representation beside being actual vectors. They are however a good starting point to play around with texts.\n",
    "\n",
    "---\n",
    "\n",
    "Define a function converting the words of a corpus to a set of dummy encoded vectors. Do not forget to sort your vocabulary before assigning the vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37d87b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:00.875037Z",
     "start_time": "2021-10-25T12:59:00.859997Z"
    }
   },
   "outputs": [],
   "source": [
    "def dummy_encode(corpus):\n",
    "    \"\"\"One-hot encoding of a set of texts.\"\"\"\n",
    "    \n",
    "    words = sorted(list(set(itertools.chain.from_iterable(corpus))))\n",
    "    embeddings = np.eye(len(words))\n",
    "    \n",
    "    return {word: embeddings[i] for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33031f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:01.946685Z",
     "start_time": "2021-10-25T12:59:01.624051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1960s': array([1., 0., 0., ..., 0., 0., 0.]),\n",
      " '<END>': array([0., 1., 0., ..., 0., 0., 0.]),\n",
      " '<START>': array([0., 0., 1., ..., 0., 0., 0.]),\n",
      " 'a': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'abnormal': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'abnormally': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'about': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'abroad': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'absence': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'accept': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'accident': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'according': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'account': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'accounted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'accu': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'across': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'action': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'added': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'adding': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'adequate': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'advised': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'affect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'affected': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'affecting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'african': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'after': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'again': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'against': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'agency': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'agreed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'agreement': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'agreements': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'agricultural': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'agriculture': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'aground': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'aid': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'air': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'aircraft': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'alarming': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'all': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'allocated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'allow': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'almost': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'along': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'already': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'also': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'although': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'always': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ambassador': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'amounted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'amounts': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'an': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'and': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'announced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'annual': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'another': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'any': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'anything': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'appears': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'apply': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'april': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'archipelago': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'are': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'areas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'arms': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'around': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'arrive': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'article': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'as': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'asbestos': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'asia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'asian': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'asked': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'associated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'assurance': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'at': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'attain': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'auction': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'australia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'authorities': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'authority': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'autumn': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'available': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'avalanches': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'average': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'away': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'azerbaijan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'back': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bags': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'balance': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'balanced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ban': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bank': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'barely': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'barter': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bartering': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bases': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'basic': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'basis': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'be': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'beans': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bearing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'because': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'become': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'been': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'began': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'begun': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'behind': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'being': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'believed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'below': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'belt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'better': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'between': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bid': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'biggest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'billion': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'black': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'blanket': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bloc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'board': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bond': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bought': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'brazil': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'break': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'britain': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'british': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'broadcast': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'broker': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'broking': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'brooke': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'brought': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'brown': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bulk': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'bureau': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'businessmen': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'but': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'butter': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'buy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'buying': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'buys': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'by': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cabinet': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'caesium': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cakes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'calendar': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'campaign': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cannot': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'capital': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cars': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'castor': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cattle': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cause': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'caused': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'causing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cautious': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ceilings': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'celsius': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'central': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'century': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cereals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'certainly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ceylon': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'chairman': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'chance': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'check': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'chemicals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'chernobyl': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'child': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'china': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'christ': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'christmas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cigarettes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cities': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'citizenship': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cla': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'clause': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'close': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'closely': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'clouds': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'coast': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'coastal': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cocoa': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'coffee': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'coincided': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cold': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'collides': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cologne': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'colombo': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'come': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'comes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'coming': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'commerce': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'commercial': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'commission': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'committee': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'commodities': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'communist': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'community': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'companies': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'compared': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'compares': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'competititve': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'complaining': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'complex': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'component': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'concern': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'conducted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'confirmed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'confirms': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'congress': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'conserve': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'considerably': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'constraints': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'consultancy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'consumed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'contained': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'continue': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'contract': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'control': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'controls': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'convertible': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'copra': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'corn': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'corp': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'corporate': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'corporation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'corporations': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'costs': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cotton': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'countertrade': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'countertrading': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'countries': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'country': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'covered': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'create': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'crop': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'crops': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'crowded': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'crude': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'crushed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'crushing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ctc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'curled': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'currency': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'current': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cut': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'cwc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'daily': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'damage': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'damaged': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'damaging': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'danger': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'data': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'date': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'day': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'deals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'debt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'december': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'decision': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'decline': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'declined': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'declines': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'deficit': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'deflect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'degrees': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'delhi': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'deltas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'demand': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'demands': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'depend': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'depending': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'designed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'despite': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'destroyed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'detailed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'details': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'detected': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'devaluation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'devastated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'develops': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'did': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'didn': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'different': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'difficult': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'difficulty': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'diminishing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dip': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'disappointed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'discontinued': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'discreetly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'discretionary': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'disease': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'disrupt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'disrupted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'diverse': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'diversify': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dlrs': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'do': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'does': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'doing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'domestic': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dong': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'down': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dramatic': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'drawdown': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'draws': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'drilling': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'drive': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'drop': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'drought': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'droughts': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dry': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'due': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'during': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'dynamism': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'each': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'earlier': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'early': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'earnings': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'eastern': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'easy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'eating': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'economic': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'edition': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'effect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'effects': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'effort': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'egypt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'eight': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'either': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'el': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'electric': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'electronic': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'elevated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'eligible': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'embassies': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'embassy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'end': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ended': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'engineering': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'enquiry': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'entitlements': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'equilibrium': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'equipment': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'estimate': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'estimated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'estimates': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'evacuated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'evacuation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'even': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'event': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'excess': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'exchange': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'exchanges': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'exclude': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'excluded': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'expected': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'expedite': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'experienced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'export': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'exporters': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'exports': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'extent': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fabricated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'face': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'faced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'factors': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fall': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'falling': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fao': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'far': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'farmers': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'farming': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'farms': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fear': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'february': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fertile': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fibre': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'figure': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'figures': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'financial': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'find': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'firm': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'firms': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fiscal': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'five': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'flexible': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'flood': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'floods': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'food': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'for': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'forced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'forecast': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'forecasting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'forecasts': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'foreign': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'formed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'found': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'free': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'freight': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'frequently': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'from': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'fruit': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'further': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'future': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gain': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gains': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gap': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gardens': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'garments': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gatt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gave': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'gems': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'general': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'geographically': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'georgia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'georgian': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'german': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'get': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'give': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'given': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'global': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'goods': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'government': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'green': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'groups': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'groves': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'grow': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'growers': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'growing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'growth': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'had': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'half': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'haq': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'harbin': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hard': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'harvests': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'has': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'have': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hazelnuts': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'he': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'health': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hearing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'heat': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'heavy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hectares': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'heightened': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'held': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'help': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'helping': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'henan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'here': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'high': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'higher': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'highlands': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hit': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'holders': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'honey': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hong': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hoppers': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'house': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'households': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'houses': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'however': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hundreds': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hurt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'husain': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'husks': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'hydro': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'if': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'illustrating': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'imbalance': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'impetus': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'import': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'importer': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'importers': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'imports': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'improve': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'in': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'inc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'include': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'including': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'incorrect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'increase': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'increased': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'increases': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'india': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'indian': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'indonesia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'indonesian': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'industries': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'industry': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'inherent': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'inquiry': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'insect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'insecticides': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'insignificant': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'interested': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'interests': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'international': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'into': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'introduction': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'inundated': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'invest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'investigation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'involving': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'iron': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'irtiza': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'is': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'it': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'its': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'izvestia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'january': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'jobs': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'joerg': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'july': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'june': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'just': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'jute': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kastl': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kenya': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kenyan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kernels': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'key': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'killed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kilo': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kilograms': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kilometres': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kind': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'known': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'kong': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'korean': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'koreans': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'laboratory': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'land': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lanka': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lankan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'large': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'largely': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'largest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'last': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'late': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'latest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'launch': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'launched': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'law': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'least': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'leather': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'leaves': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'less': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lesser': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'letter': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'level': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'levels': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'licences': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'likely': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'limited': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'limits': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lines': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'link': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'linked': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'linking': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'links': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'liptons': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'little': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'local': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'long': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'longer': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'look': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'losses': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'low': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lower': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lowered': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lowest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'lt': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'machinery': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'made': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mahbubul': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'main': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mainly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'major': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'make': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'manufactured': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'many': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'march': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'market': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'markets': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'matter': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'maximum': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'may': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'meal': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'measure': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'measures': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'media': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'meet': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'member': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'members': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'memory': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'metals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'meteorological': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'meteorologists': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mica': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mid': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'milk': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'million': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mineral': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'minerals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mingles': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'minimum': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'minister': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ministry': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mln': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mmtc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'modest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mombasa': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'monday': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'monitoring': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'months': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'more': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'moscow': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'most': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'mountain': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'move': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'much': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nairobi': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'narrow': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nations': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'natural': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'need': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'negative': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'netherlands': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'new': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'news': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'newspaper': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nhan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nine': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nino': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'no': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'non': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'normal': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'normally': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'northeast': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'not': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'noted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nothing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'now': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'nuclear': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'occur': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'occurrence': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ocean': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'oct': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'october': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'of': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'off': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'offer': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'offered': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'offical': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'official': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'officially': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'officials': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'offset': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'often': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'oil': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'on': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'one': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'only': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'opened': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'opium': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'or': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'orange': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'order': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ordered': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ore': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ores': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'organisation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'origin': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'other': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'our': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'outbreak': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'outlets': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'outlining': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'output': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'over': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'overall': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'overseas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pacific': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pack': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'paid': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pakistan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pakistani': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'palm': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'paper': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'part': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'particular': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'parties': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'partners': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'parts': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'party': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'past': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pattern': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'patterns': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'payment': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'payments': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pct': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'peking': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'people': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pepper': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'performance': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'permit': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'permits': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'permitted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'peru': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pests': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'petroleum': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pharmaceuticals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'phenomenon': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'places': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'planning': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plans': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plant': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plantation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plantations': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'planters': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'planting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plants': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plastics': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'platform': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'plywood': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'po': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'point': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'policy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pork': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pose': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'possible': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'power': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'prayer': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'precautionary': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'precise': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'preference': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'prescribed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'present': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'press': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pressure': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'preventive': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'previous': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'prices': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'private': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'probable': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'problem': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'problems': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'procedures': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'processed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'processing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'produce': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'produced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'producing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'product': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'production': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'products': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'projected': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promised': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promising': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promote': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promoted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promoting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promotion': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'promotional': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pronounced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'prospect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'prospects': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'protectionism': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'protest': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'protocol': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'provide': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'provided': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'provides': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'providing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'provinces': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'psyllium': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'pump': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'purchase': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'purposes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'push': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'put': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'qualify': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'qualities': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'quality': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'queensland': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'quietly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'quotas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'radiation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rails': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'railway': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rain': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rainfall': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rains': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rainy': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'raise': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rate': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'raw': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rbi': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 're': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reached': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reaches': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'readings': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ready': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'received': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'recent': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'recently': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'recommended': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'record': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'recorded': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reduce': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reduced': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'region': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'regulations': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reintensification': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'relaxes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'release': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'relied': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'relocation': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reluctance': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'remained': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'remaining': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'remains': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'repayment': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'replace': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'replaces': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'report': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reported': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reports': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'republic': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'requested': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'requirement': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reserve': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reserves': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'resettle': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'resettled': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'residents': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'resistant': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'respect': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'respective': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'responsible': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'restrictive': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'result': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'resulted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'resumed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'return': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'returns': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'reuters': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'revenue': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rice': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rights': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rigs': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rise': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rising': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'river': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rivers': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'roads': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'role': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'room': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'roubles': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rubber': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rule': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rules': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'run': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rupees': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'rupiah': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'russia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 's': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'said': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'same': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'samples': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sandalwood': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'saying': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'says': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'scale': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'schedule': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'scheme': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'scientists': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sea': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'searching': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'seas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'season': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'second': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'secondary': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sector': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'seeds': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'seek': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'seeking': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'seen': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'selected': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sell': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'selling': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'semi': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sends': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sent': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'separate': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sept': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'september': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'serious': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'service': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'services': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'set': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'seven': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'several': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'severe': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shift': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shifting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shillings': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shipping': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ships': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shops': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'short': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shortage': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shortages': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shortly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'should': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'show': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'showed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'showing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shown': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'shrinking': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sichuan': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'signals': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'signed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'significant': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'significantly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'since': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'single': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'six': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'slight': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'slightly': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'slow': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'slump': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'small': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'snows': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'so': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sold': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'some': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sought': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'source': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sources': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'south': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'southern': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'soviet': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sowing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'soybean': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'soybeans': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spanish': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'specialises': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'speed': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spending': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spent': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spokesman': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spokeswoman': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sports': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spotlight': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'spring': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sri': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stage': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stages': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stagnates': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'staple': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'start': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'state': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'statement': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'states': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stc': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'steadily': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'steel': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'steps': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'still': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stocks': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stood': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stopped': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stored': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'stream': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'strengthened': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'strike': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'striken': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'strip': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'structures': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'studied': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'such': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sudden': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sugarcane': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'sum': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'supply': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'support': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'surge': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'surprise': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'suspend': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'suspended': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'swaps': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 't': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'take': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'taken': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'targetted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tariffs': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tcp': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tea': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'teas': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'telephone': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'television': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'temperature': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'temperatures': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'temples': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'temporary': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tenders': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'term': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'testing': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tests': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'textiles': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'than': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'that': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'thaw': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'the': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'their': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'them': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'then': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'these': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'they': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'third': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'this': array([0., 0., 0., ..., 0., 0., 0.]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'thousands': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'threat': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'threatened': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'threatens': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'three': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'through': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tight': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tightened': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'time': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'to': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tobacco': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'today': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'told': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tonnes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'took': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'torn': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'total': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'trade': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'traders': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'trading': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'trend': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'tries': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'turnover': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'two': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'types': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'u': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ukraine': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'ukrainian': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'under': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'underlines': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'understood': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'unilever': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'union': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'unique': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'united': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'units': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'university': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'unless': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'until': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'unusually': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'up': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'upon': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'upper': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'use': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'used': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'usual': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'usually': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'value': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'varieties': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'various': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'vary': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'vegetables': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'venezuela': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'verbal': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'very': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'vietnam': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'violent': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'volumes': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'voting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wait': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'waiting': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wanted': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wants': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'warm': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'warmer': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'warned': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'was': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'water': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'waters': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'we': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'weather': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'week': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'weeks': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wei': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'well': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wen': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'went': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'were': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wereng': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'west': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'western': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wet': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'wheat': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'when': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'where': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'whether': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'which': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'while': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'who': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'why': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'will': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'winds': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'winter': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'with': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'without': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'workers': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'world': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'worst': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'worth': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'would': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'yangtze': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'year': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'years': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'yesterday': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'yet': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'yielding': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'yields': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'yugoslavia': array([0., 0., 0., ..., 0., 0., 0.]),\n",
      " 'zero': array([0., 0., 0., ..., 1., 0., 0.]),\n",
      " 'zimbabwe': array([0., 0., 0., ..., 0., 1., 0.]),\n",
      " 'zones': array([0., 0., 0., ..., 0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "pp(dummy_encode(corpus), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40953475",
   "metadata": {},
   "source": [
    "If you still do not believe that this representation is pointless, try finding the most similar word to \"cat\" using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212367c",
   "metadata": {},
   "source": [
    "## Co-occurence matrix encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d7908",
   "metadata": {},
   "source": [
    "*This section comes from Stanford's NLP hands-on*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830989b",
   "metadata": {},
   "source": [
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word  $w_i$  occurring in the document, we consider the context window surrounding  $w_i$ . Supposing our fixed window size is  $n$ , then this is the  $n$  preceding and  $n$  subsequent words in that document, i.e. words  $w_{in}  w_{i1}$  and  $w_{i+1}  w{i+n}$ . We build a co-occurrence matrix  $M$ , which is a symmetric word-by-word matrix in which  $M_{ij}$  is the number of times  $w_j$  appears inside  $w_i$ 's window among all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66869c4",
   "metadata": {},
   "source": [
    "**Example: Co-Occurrence with Fixed Window of n=1:**\n",
    "\n",
    "* Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "* Document 2: \"all is well that ends well\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f2b4c",
   "metadata": {},
   "source": [
    "|         \t| START \t| all \t| that \t| glitters \t| is \t| not \t| gold \t| well \t| ends \t| END \t|\n",
    "|---------:\t|--------:\t|----:\t|-----:\t|---------:\t|---:\t|----:\t|-----:\t|-----:\t|-----:\t|------:\t|\n",
    "|  START \t|       0 \t|   2 \t|    0 \t|        0 \t|  0 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|      all \t|       2 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|     that \t|       0 \t|   1 \t|    0 \t|        1 \t|  0 \t|   0 \t|    0 \t|    1 \t|    1 \t|     0 \t|\n",
    "| glitters \t|       0 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|       is \t|       0 \t|   1 \t|    0 \t|        1 \t|  0 \t|   1 \t|    0 \t|    1 \t|    0 \t|     0 \t|\n",
    "|      not \t|       0 \t|   0 \t|    0 \t|        0 \t|  1 \t|   0 \t|    1 \t|    0 \t|    0 \t|     0 \t|\n",
    "|     gold \t|       0 \t|   0 \t|    0 \t|        0 \t|  0 \t|   1 \t|    0 \t|    0 \t|    0 \t|     1 \t|\n",
    "|     well \t|       0 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    1 \t|     1 \t|\n",
    "|     ends \t|       0 \t|   0 \t|    1 \t|        0 \t|  0 \t|   0 \t|    0 \t|    1 \t|    0 \t|     0 \t|\n",
    "|    END \t|       0 \t|   0 \t|    0 \t|        0 \t|  0 \t|   0 \t|    1 \t|    1 \t|    0 \t|     0 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b153b8",
   "metadata": {},
   "source": [
    "The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run dimensionality reduction. In particular, we will run *SVD* (Singular Value Decomposition), which is a kind of generalized *PCA* (Principal Components Analysis) to select the top  k  principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc6263",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of such vectors doesn't interterfere with the semantic relationship between words. Hence, *movie* will still be closer to *theater* than to *airplane*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41120cd",
   "metadata": {},
   "source": [
    "### Identify distinct words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf508598",
   "metadata": {},
   "source": [
    "Define a function that will return a list of unique words of the corpus as well as its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a376f246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:11.098237Z",
     "start_time": "2021-10-25T12:59:11.086180Z"
    }
   },
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = sorted(list(set(itertools.chain.from_iterable(corpus))))\n",
    "\n",
    "    return corpus_words, len(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d9298b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:11.286051Z",
     "start_time": "2021-10-25T12:59:11.268780Z"
    }
   },
   "outputs": [],
   "source": [
    "distincts, num_words = distinct_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce9e6d",
   "metadata": {},
   "source": [
    "### Compute the co-occurence matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bda31",
   "metadata": {},
   "source": [
    "Write a method that constructs a co-occurrence matrix for a certain window-size  $n$  (with a default of $4$), considering words  $n$  before and  $n$  after the word in the center of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1760c654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:35.255094Z",
     "start_time": "2021-10-25T12:59:35.237100Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
    "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = np.zeros((num_words, num_words))\n",
    "    word2ind = {w: i for i, w in enumerate(words)}\n",
    "    \n",
    "    base_occurence_count = {w: 0 for w in words}\n",
    "    occurence_counts = []\n",
    "    \n",
    "    for text in corpus:\n",
    "        for i, central_word in enumerate(text):\n",
    "            window = text[max(0, i-window_size):i] + text[i+1:min(len(text), window_size+i)+1]\n",
    "            for context_word in window:\n",
    "                M[word2ind[central_word]][word2ind[context_word]] += 1\n",
    "\n",
    "    return M, word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04c130c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:36.528968Z",
     "start_time": "2021-10-25T12:59:36.490151Z"
    }
   },
   "outputs": [],
   "source": [
    "co_matrix, word_idx = compute_co_occurrence_matrix(corpus, window_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1596792d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:38.068393Z",
     "start_time": "2021-10-25T12:59:38.036997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <th>all</th>\n",
       "      <th>that</th>\n",
       "      <th>glitters</th>\n",
       "      <th>is</th>\n",
       "      <th>not</th>\n",
       "      <th>gold</th>\n",
       "      <th>well</th>\n",
       "      <th>ends</th>\n",
       "      <th>&lt;END&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;START&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glitters</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ends</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;END&gt;</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          <START>  all  that  glitters  is  not  gold  well  ends  <END>\n",
       "<START>         0    2     0         0   0    0     0     0     0      0\n",
       "all             2    0     1         0   1    0     0     0     0      0\n",
       "that            0    1     0         1   0    0     0     1     1      0\n",
       "glitters        0    0     1         0   1    0     0     0     0      0\n",
       "is              0    1     0         1   0    1     0     1     0      0\n",
       "not             0    0     0         0   1    0     1     0     0      0\n",
       "gold            0    0     0         0   0    1     0     0     0      1\n",
       "well            0    0     1         0   1    0     0     0     1      1\n",
       "ends            0    0     1         0   0    0     0     1     0      0\n",
       "<END>           0    0     0         0   0    0     1     1     0      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs_check = [\n",
    "    \"<START> all that glitters is not gold <END>\".split(\" \"),\n",
    "    \"<START> all is well that ends well <END>\".split(\" \")\n",
    "]\n",
    "\n",
    "co_matrix_test, word_idx_test = compute_co_occurrence_matrix(docs_check, window_size=1)\n",
    "\n",
    "# Display to match the above matrix in the example\n",
    "words = word_idx_test.keys()\n",
    "co_matrix_test = pd.DataFrame(co_matrix_test, columns=words, index=words)\n",
    "co_matrix_test = co_matrix_test.reindex([\"<START>\", \"all\" , \"that\", \"glitters\", \"is\", \"not\", \"gold\", \"well\", \"ends\", \"<END>\"])\n",
    "co_matrix_test = co_matrix_test[[\"<START>\", \"all\" , \"that\", \"glitters\", \"is\", \"not\", \"gold\", \"well\", \"ends\", \"<END>\"]]\n",
    "display(co_matrix_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14ce49",
   "metadata": {},
   "source": [
    "### Reduce the dimensionality of the co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b28d3f",
   "metadata": {},
   "source": [
    "Construct a method that performs dimensionality reduction on the matrix to produce $k$-dimensional embeddings. Use *SVD* to take the top $k$ components and produce a new matrix of $k$-dimensional embeddings.\n",
    "\n",
    "In our case, we will set $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89c3f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:56.290773Z",
     "start_time": "2021-10-25T12:59:56.281685Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    svd = TruncatedSVD(n_components = k, n_iter = n_iters)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0b85e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:57.127389Z",
     "start_time": "2021-10-25T12:59:57.056652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 1140 words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "reduced_co_matrix = reduce_to_k_dim(co_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a8ee3aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:59:58.864231Z",
     "start_time": "2021-10-25T12:59:58.851094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80484324, -0.5139259 ],\n",
       "       [ 5.19968129, -0.6825013 ],\n",
       "       [ 2.37395942,  0.36485983],\n",
       "       ...,\n",
       "       [ 0.62509759,  0.2798576 ],\n",
       "       [ 0.91626467,  0.53682652],\n",
       "       [ 2.53154089,  0.81019438]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1938d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2097a",
   "metadata": {},
   "source": [
    "Great! You now have fix-sized vectors that represent each words of your corpus. Let's normalize our matrix to compare our vectors easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00217789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:00:01.492027Z",
     "start_time": "2021-10-25T13:00:01.473988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "co_matrix_lengths = np.linalg.norm(reduced_co_matrix, axis=1)\n",
    "co_matrix_normalized = reduced_co_matrix / co_matrix_lengths[:, np.newaxis] # broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458ea3d",
   "metadata": {},
   "source": [
    "Since we are working with vectors, we can easily measure the similarity between them using the dot product. Hence, given a specific word and its related word embedding, we can easily identify its most similar words contained in the corpus!\n",
    "\n",
    "*Note: you can either use a dot product or a cosine similarity here. The dot product cares about both angle and magnitude between the vectors while the cosine similarity only care about their angle.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729e4e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T09:48:08.969468Z",
     "start_time": "2021-10-25T09:48:08.950859Z"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d8e61",
   "metadata": {},
   "source": [
    "Let's create a dictionary that maps a vector to a its representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d13c806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:00:04.259803Z",
     "start_time": "2021-10-25T13:00:04.249859Z"
    }
   },
   "outputs": [],
   "source": [
    "svd_word_vectors = {word: co_matrix_normalized[i] for i, word in enumerate(word_idx.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9f96d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:00:12.338212Z",
     "start_time": "2021-10-25T13:00:12.259673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1960s': array([ 0.84282903, -0.53818141]),\n",
       " '<END>': array([ 0.99149537, -0.13014199]),\n",
       " '<START>': array([0.98839451, 0.15190885]),\n",
       " 'a': array([0.97393753, 0.22681641]),\n",
       " 'abnormal': array([ 0.95730113, -0.28909264]),\n",
       " 'abnormally': array([0.80698539, 0.5905714 ]),\n",
       " 'about': array([0.98288048, 0.18424432]),\n",
       " 'abroad': array([ 0.8397594, -0.5429587]),\n",
       " 'absence': array([0.99900643, 0.04456618]),\n",
       " 'accept': array([0.82489571, 0.56528495]),\n",
       " 'accident': array([ 0.96867693, -0.2483244 ]),\n",
       " 'according': array([0.99652457, 0.08329932]),\n",
       " 'account': array([0.94803702, 0.31816002]),\n",
       " 'accounted': array([ 0.95682546, -0.29066311]),\n",
       " 'accu': array([0.99277676, 0.11997625]),\n",
       " 'across': array([ 0.93951921, -0.34249622]),\n",
       " 'action': array([ 0.98241391, -0.18671611]),\n",
       " 'added': array([0.9933149 , 0.11543613]),\n",
       " 'adding': array([0.95983326, 0.28057104]),\n",
       " 'adequate': array([0.99388343, 0.11043423]),\n",
       " 'advised': array([0.88969081, 0.45656354]),\n",
       " 'affect': array([ 0.7476838 , -0.66405492]),\n",
       " 'affected': array([ 0.99269271, -0.12066972]),\n",
       " 'affecting': array([0.79931513, 0.60091207]),\n",
       " 'african': array([0.87846087, 0.4778143 ]),\n",
       " 'after': array([ 0.81227521, -0.58327436]),\n",
       " 'again': array([0.95404793, 0.29965404]),\n",
       " 'against': array([ 0.99796211, -0.06380933]),\n",
       " 'agency': array([ 0.83654867, -0.54789262]),\n",
       " 'agreed': array([0.98005177, 0.19874236]),\n",
       " 'agreement': array([ 0.98060149, -0.19601205]),\n",
       " 'agreements': array([0.77578191, 0.63100113]),\n",
       " 'agricultural': array([0.96148723, 0.2748496 ]),\n",
       " 'agriculture': array([0.95177417, 0.30679949]),\n",
       " 'aground': array([0.72275072, 0.69110882]),\n",
       " 'aid': array([0.97192104, 0.23530723]),\n",
       " 'air': array([ 0.96199469, -0.27306815]),\n",
       " 'aircraft': array([0.8899381 , 0.45608133]),\n",
       " 'alarming': array([0.99996354, 0.00853908]),\n",
       " 'all': array([0.85139869, 0.52451908]),\n",
       " 'allocated': array([ 0.91850762, -0.39540327]),\n",
       " 'allow': array([0.78470979, 0.61986333]),\n",
       " 'almost': array([0.7718251, 0.6358349]),\n",
       " 'along': array([ 0.86367781, -0.50404429]),\n",
       " 'already': array([0.94463784, 0.32811485]),\n",
       " 'also': array([0.97713995, 0.21259706]),\n",
       " 'although': array([0.91279521, 0.40841756]),\n",
       " 'always': array([ 0.89022659, -0.45551796]),\n",
       " 'ambassador': array([0.52167802, 0.85314245]),\n",
       " 'amounted': array([ 0.99944549, -0.03329738]),\n",
       " 'amounts': array([0.81887392, 0.57397343]),\n",
       " 'an': array([0.97137316, 0.23755879]),\n",
       " 'and': array([0.9998379 , 0.01800493]),\n",
       " 'announced': array([ 0.99281903, -0.11962599]),\n",
       " 'annual': array([0.98321558, 0.18244757]),\n",
       " 'another': array([ 0.78713874, -0.61677598]),\n",
       " 'any': array([0.99987759, 0.01564601]),\n",
       " 'anything': array([0.74760106, 0.66414807]),\n",
       " 'appears': array([0.9556641 , 0.29445903]),\n",
       " 'apply': array([0.88595446, 0.46377225]),\n",
       " 'april': array([0.99790466, 0.06470158]),\n",
       " 'archipelago': array([ 0.91171915, -0.41081407]),\n",
       " 'are': array([0.99999656, 0.00262346]),\n",
       " 'areas': array([ 0.99756875, -0.06968917]),\n",
       " 'arms': array([ 0.9999867 , -0.00515816]),\n",
       " 'around': array([0.96765297, 0.25228503]),\n",
       " 'arrive': array([0.86122885, 0.50821735]),\n",
       " 'article': array([ 0.9497485 , -0.31301404]),\n",
       " 'as': array([ 0.97833958, -0.20700646]),\n",
       " 'asbestos': array([0.87820437, 0.47828557]),\n",
       " 'asia': array([ 0.98231773, -0.18722146]),\n",
       " 'asian': array([ 0.72141326, -0.69250481]),\n",
       " 'asked': array([0.88529608, 0.46502779]),\n",
       " 'associated': array([ 0.77676672, -0.62978842]),\n",
       " 'assurance': array([0.99266106, 0.12092982]),\n",
       " 'at': array([ 0.97612947, -0.21718945]),\n",
       " 'attain': array([ 0.92065367, -0.39038035]),\n",
       " 'auction': array([ 0.98437182, -0.17610257]),\n",
       " 'australia': array([0.97587417, 0.21833369]),\n",
       " 'authorities': array([ 0.98544305, -0.17000588]),\n",
       " 'authority': array([ 0.93181431, -0.36293538]),\n",
       " 'autumn': array([ 0.94587549, -0.32452976]),\n",
       " 'available': array([ 0.77941164, -0.62651217]),\n",
       " 'avalanches': array([ 0.99647983, -0.08383281]),\n",
       " 'average': array([ 0.99435131, -0.10613896]),\n",
       " 'away': array([ 0.79186941, -0.61069046]),\n",
       " 'azerbaijan': array([0.97398477, 0.22661348]),\n",
       " 'back': array([ 0.98968983, -0.14322722]),\n",
       " 'bags': array([0.98200788, 0.18883993]),\n",
       " 'balance': array([ 0.94549982, -0.32562261]),\n",
       " 'balanced': array([0.66971861, 0.74261497]),\n",
       " 'ban': array([ 0.93230054, -0.36168455]),\n",
       " 'bank': array([ 0.92702784, -0.3749925 ]),\n",
       " 'barely': array([ 0.83913885, -0.54391727]),\n",
       " 'barter': array([0.95760511, 0.2880841 ]),\n",
       " 'bartering': array([ 0.75203753, -0.65912029]),\n",
       " 'bases': array([0.99961342, 0.02780316]),\n",
       " 'basic': array([ 0.86945353, -0.49401473]),\n",
       " 'basis': array([ 0.92699222, -0.37508055]),\n",
       " 'be': array([ 0.99861827, -0.05255044]),\n",
       " 'beans': array([ 0.98315146, -0.1827928 ]),\n",
       " 'bearing': array([ 0.99385381, -0.11070053]),\n",
       " 'because': array([ 0.98966679, -0.14338638]),\n",
       " 'become': array([0.99099181, 0.1339225 ]),\n",
       " 'been': array([0.99750808, 0.07055233]),\n",
       " 'began': array([0.89951067, 0.4368988 ]),\n",
       " 'begun': array([ 0.99010872, -0.14030229]),\n",
       " 'behind': array([ 0.9996296 , -0.02721507]),\n",
       " 'being': array([0.86369654, 0.50401219]),\n",
       " 'believed': array([ 0.8262028 , -0.56337282]),\n",
       " 'below': array([ 0.98571507, -0.16842149]),\n",
       " 'belt': array([ 0.84349056, -0.537144  ]),\n",
       " 'better': array([ 0.87108546, -0.49113147]),\n",
       " 'between': array([ 0.94596117, -0.32427992]),\n",
       " 'bid': array([ 0.76859997, -0.63972969]),\n",
       " 'biggest': array([ 0.97987826, -0.19959607]),\n",
       " 'billion': array([0.96581846, 0.25921942]),\n",
       " 'black': array([0.9656322 , 0.25991241]),\n",
       " 'blanket': array([ 0.77361572, -0.63365504]),\n",
       " 'bloc': array([ 0.99000206, -0.1410529 ]),\n",
       " 'board': array([0.97030925, 0.24186767]),\n",
       " 'bond': array([0.93475392, 0.3552958 ]),\n",
       " 'bought': array([0.87963746, 0.47564476]),\n",
       " 'brazil': array([ 0.99512632, -0.09860839]),\n",
       " 'break': array([ 0.99835775, -0.05728696]),\n",
       " 'britain': array([0.94612031, 0.32381532]),\n",
       " 'british': array([0.77662703, 0.62996068]),\n",
       " 'broadcast': array([0.80243623, 0.59673788]),\n",
       " 'broker': array([0.86078829, 0.50896318]),\n",
       " 'broking': array([0.98475295, 0.17395872]),\n",
       " 'brooke': array([0.89712408, 0.44177867]),\n",
       " 'brought': array([0.9293529, 0.3691926]),\n",
       " 'brown': array([0.76735073, 0.64122762]),\n",
       " 'bulk': array([0.84834771, 0.52943948]),\n",
       " 'bureau': array([0.71063043, 0.70356549]),\n",
       " 'businessmen': array([0.71946135, 0.69453248]),\n",
       " 'but': array([ 0.99453305, -0.10442229]),\n",
       " 'butter': array([0.96737048, 0.25336603]),\n",
       " 'buy': array([0.97763901, 0.21029022]),\n",
       " 'buying': array([0.8584024 , 0.51297693]),\n",
       " 'buys': array([0.786363  , 0.61776471]),\n",
       " 'by': array([ 0.9862657, -0.1651665]),\n",
       " 'cabinet': array([0.94742852, 0.31996749]),\n",
       " 'caesium': array([0.99699916, 0.07741235]),\n",
       " 'cakes': array([ 0.8610223, -0.5085672]),\n",
       " 'calendar': array([0.90638872, 0.42244465]),\n",
       " 'campaign': array([0.9948844 , 0.10101999]),\n",
       " 'cannot': array([ 0.86088098, -0.50880639]),\n",
       " 'capital': array([ 0.96939247, -0.24551627]),\n",
       " 'cars': array([ 0.98663072, -0.16297182]),\n",
       " 'castor': array([ 0.99771916, -0.06750166]),\n",
       " 'cattle': array([ 0.94169948, -0.33645518]),\n",
       " 'cause': array([0.93299646, 0.35988553]),\n",
       " 'caused': array([0.92996948, 0.36763673]),\n",
       " 'causing': array([0.92706748, 0.37489451]),\n",
       " 'cautious': array([ 0.93270049, -0.36065189]),\n",
       " 'ceilings': array([0.99836851, 0.05709912]),\n",
       " 'celsius': array([0.91516484, 0.40307979]),\n",
       " 'central': array([0.99823627, 0.05936625]),\n",
       " 'century': array([0.89755514, 0.44090223]),\n",
       " 'cereals': array([ 0.98670968, -0.16249312]),\n",
       " 'certainly': array([0.93445366, 0.35608477]),\n",
       " 'ceylon': array([0.99980075, 0.0199615 ]),\n",
       " 'chairman': array([ 0.7152184 , -0.69890102]),\n",
       " 'chance': array([ 0.74669002, -0.66517217]),\n",
       " 'check': array([0.96389676, 0.26627625]),\n",
       " 'chemicals': array([0.99294429, 0.11858175]),\n",
       " 'chernobyl': array([ 0.75147565, -0.65976082]),\n",
       " 'child': array([0.97124624, 0.23807718]),\n",
       " 'china': array([ 0.95671517, -0.29102591]),\n",
       " 'christ': array([0.95717113, 0.28952276]),\n",
       " 'christmas': array([ 0.85595018, -0.51705831]),\n",
       " 'cigarettes': array([ 0.98159586, -0.19097007]),\n",
       " 'cities': array([ 0.9641467 , -0.26536983]),\n",
       " 'citizenship': array([0.99987427, 0.01585681]),\n",
       " 'cla': array([ 0.79161083, -0.61102561]),\n",
       " 'clause': array([ 0.93334659, -0.35897652]),\n",
       " 'close': array([0.99956286, 0.02956488]),\n",
       " 'closely': array([0.83187291, 0.55496618]),\n",
       " 'clouds': array([ 0.94914937, -0.31482611]),\n",
       " 'coast': array([ 0.99322667, -0.11619282]),\n",
       " 'coastal': array([0.82157499, 0.57010047]),\n",
       " 'cocoa': array([0.9998707 , 0.01608041]),\n",
       " 'coffee': array([0.9978718 , 0.06520635]),\n",
       " 'coincided': array([ 0.89341771, -0.44922689]),\n",
       " 'cold': array([ 0.79998037, -0.60002617]),\n",
       " 'collides': array([ 0.77189024, -0.63575581]),\n",
       " 'cologne': array([0.9145564, 0.4044584]),\n",
       " 'colombo': array([ 0.78227506, -0.62293316]),\n",
       " 'come': array([ 0.9634224 , -0.26798745]),\n",
       " 'comes': array([0.86641346, 0.49932726]),\n",
       " 'coming': array([ 0.97597568, -0.21787949]),\n",
       " 'commerce': array([0.99995759, 0.00920953]),\n",
       " 'commercial': array([0.84197502, 0.5395165 ]),\n",
       " 'commission': array([ 0.97137381, -0.23755616]),\n",
       " 'committee': array([0.98031485, 0.19744059]),\n",
       " 'commodities': array([0.99884446, 0.04805971]),\n",
       " 'communist': array([0.9842162 , 0.17697023]),\n",
       " 'community': array([ 0.90153549, -0.43270517]),\n",
       " 'companies': array([ 0.98679925, -0.16194824]),\n",
       " 'compared': array([0.95186532, 0.30651658]),\n",
       " 'compares': array([ 0.99156437, -0.12961518]),\n",
       " 'competititve': array([ 0.99713015, -0.07570647]),\n",
       " 'complaining': array([ 0.98896301, -0.14816264]),\n",
       " 'complex': array([ 0.94419953, -0.32937402]),\n",
       " 'component': array([ 0.93889915, -0.34419238]),\n",
       " 'concern': array([ 0.98959572, -0.143876  ]),\n",
       " 'conducted': array([0.95872718, 0.28432761]),\n",
       " 'confirmed': array([0.81888243, 0.57396129]),\n",
       " 'confirms': array([ 0.98664018, -0.16291459]),\n",
       " 'congress': array([0.99989524, 0.01447468]),\n",
       " 'conserve': array([0.99476546, 0.10218449]),\n",
       " 'considerably': array([ 0.84393291, -0.53644874]),\n",
       " 'constraints': array([0.97194185, 0.23522126]),\n",
       " 'consultancy': array([ 0.79978217, -0.60029033]),\n",
       " 'consumed': array([ 0.94844057, -0.31695503]),\n",
       " 'contained': array([ 0.97235341, -0.23351413]),\n",
       " 'continue': array([0.93091857, 0.3652268 ]),\n",
       " 'contract': array([ 0.84972587, -0.52722475]),\n",
       " 'control': array([0.90591815, 0.42345284]),\n",
       " 'controls': array([0.79752298, 0.60328857]),\n",
       " 'convertible': array([ 0.96981345, -0.24384805]),\n",
       " 'copra': array([0.85231505, 0.52302874]),\n",
       " 'corn': array([0.87784293, 0.47894864]),\n",
       " 'corp': array([ 0.83032956, -0.55727266]),\n",
       " 'corporate': array([ 0.99873109, -0.0503608 ]),\n",
       " 'corporation': array([ 0.96936014, -0.24564388]),\n",
       " 'corporations': array([ 0.95317883, -0.30240722]),\n",
       " 'costs': array([0.99996583, 0.0082669 ]),\n",
       " 'cotton': array([0.98935088, 0.1455501 ]),\n",
       " 'countertrade': array([0.99980976, 0.01950473]),\n",
       " 'countertrading': array([0.85036596, 0.52619173]),\n",
       " 'countries': array([0.95785323, 0.28725805]),\n",
       " 'country': array([ 0.94341343, -0.33161892]),\n",
       " 'covered': array([0.99967949, 0.02531616]),\n",
       " 'create': array([0.89710265, 0.44182219]),\n",
       " 'crop': array([ 0.9288903 , -0.37035499]),\n",
       " 'crops': array([ 0.84575591, -0.53357   ]),\n",
       " 'crowded': array([ 0.91850221, -0.39541585]),\n",
       " 'crude': array([0.94857156, 0.31656278]),\n",
       " 'crushed': array([ 0.99287248, -0.11918153]),\n",
       " 'crushing': array([ 0.80344616, -0.59537742]),\n",
       " 'ctc': array([ 0.9890435, -0.1476244]),\n",
       " 'curled': array([ 0.98653128, -0.16357274]),\n",
       " 'currency': array([0.87305745, 0.48761736]),\n",
       " 'current': array([ 0.88321739, -0.4689638 ]),\n",
       " 'cut': array([0.99027735, 0.13910705]),\n",
       " 'cwc': array([ 0.98418464, -0.17714567]),\n",
       " 'daily': array([ 0.98340546, -0.18142133]),\n",
       " 'damage': array([ 0.99892181, -0.04642423]),\n",
       " 'damaged': array([0.9057965, 0.423713 ]),\n",
       " 'damaging': array([ 0.9991067 , -0.04225866]),\n",
       " 'dan': array([0.70829086, 0.70592071]),\n",
       " 'danger': array([ 0.96796999, -0.25106592]),\n",
       " 'data': array([0.99009136, 0.14042473]),\n",
       " 'date': array([ 0.99958872, -0.02867732]),\n",
       " 'day': array([0.85941371, 0.51128081]),\n",
       " 'deals': array([ 0.96417418, -0.26526996]),\n",
       " 'debt': array([ 0.78193942, -0.62335443]),\n",
       " 'december': array([ 0.97926402, -0.20258819]),\n",
       " 'decision': array([ 0.99859903, -0.05291472]),\n",
       " 'decline': array([0.99011883, 0.14023089]),\n",
       " 'declined': array([ 0.95556346, -0.29478548]),\n",
       " 'declines': array([ 0.80516256, -0.59305418]),\n",
       " 'deficit': array([ 0.99422892, -0.10727931]),\n",
       " 'deflect': array([ 0.93462949, -0.355623  ]),\n",
       " 'degrees': array([ 0.94571128, -0.32500797]),\n",
       " 'delhi': array([ 0.9918597 , -0.12733554]),\n",
       " 'deltas': array([ 0.93631408, -0.35116369]),\n",
       " 'demand': array([0.9999694, 0.0078224]),\n",
       " 'demands': array([0.95333135, 0.30192606]),\n",
       " 'depend': array([ 0.92132629, -0.38879026]),\n",
       " 'depending': array([ 0.95681978, -0.29068179]),\n",
       " 'designed': array([ 0.94577101, -0.3248341 ]),\n",
       " 'despite': array([ 0.98786906, -0.15528915]),\n",
       " 'destroyed': array([0.93693119, 0.34951388]),\n",
       " 'detailed': array([ 0.99997496, -0.00707649]),\n",
       " 'details': array([ 0.99714512, -0.075509  ]),\n",
       " 'detected': array([ 0.81997048, -0.57240582]),\n",
       " 'devaluation': array([ 0.95456591, -0.29799987]),\n",
       " 'devastated': array([ 0.88559575, -0.46445684]),\n",
       " 'develops': array([ 0.97221865, -0.23407454]),\n",
       " 'did': array([ 0.94353682, -0.33126766]),\n",
       " 'didn': array([ 0.92575411, -0.37812607]),\n",
       " 'different': array([0.78250083, 0.62264954]),\n",
       " 'difficult': array([0.90746155, 0.42013515]),\n",
       " 'difficulty': array([0.81901718, 0.57376899]),\n",
       " 'diminishing': array([ 0.84648691, -0.53240954]),\n",
       " 'dip': array([0.93722832, 0.34871633]),\n",
       " 'disappointed': array([0.99283279, 0.11951171]),\n",
       " 'discontinued': array([0.7036498 , 0.71054694]),\n",
       " 'discreetly': array([ 0.71327413, -0.70088517]),\n",
       " 'discretionary': array([ 0.91101287, -0.41237792]),\n",
       " 'disease': array([0.90346924, 0.42865294]),\n",
       " 'disrupt': array([ 0.77601398, -0.6307157 ]),\n",
       " 'disrupted': array([0.88578131, 0.46410287]),\n",
       " 'diverse': array([0.80682169, 0.59079502]),\n",
       " 'diversify': array([0.90574184, 0.42382983]),\n",
       " 'dlrs': array([0.99310299, 0.11724524]),\n",
       " 'do': array([0.82975242, 0.55813163]),\n",
       " 'does': array([ 0.92942799, -0.36900353]),\n",
       " 'doing': array([ 0.99034766, -0.13860562]),\n",
       " 'domestic': array([0.98001705, 0.19891349]),\n",
       " 'dong': array([ 0.92355455, -0.38346706]),\n",
       " 'down': array([0.93749776, 0.3479913 ]),\n",
       " 'dramatic': array([0.99987969, 0.01551148]),\n",
       " 'drawdown': array([0.85810288, 0.51347779]),\n",
       " 'draws': array([ 9.99999970e-01, -2.44473102e-04]),\n",
       " 'drilling': array([0.96616201, 0.25793598]),\n",
       " 'drive': array([ 0.97731805, -0.21177682]),\n",
       " 'drop': array([ 0.8385344 , -0.54484865]),\n",
       " 'drought': array([ 0.89921629, -0.43750437]),\n",
       " 'droughts': array([ 0.99826852, -0.05882144]),\n",
       " 'dry': array([ 0.87530693, -0.48356777]),\n",
       " 'due': array([0.85506639, 0.51851853]),\n",
       " 'during': array([ 0.97583081, -0.2185274 ]),\n",
       " 'dynamism': array([0.90466067, 0.42613269]),\n",
       " 'each': array([0.82266577, 0.56852531]),\n",
       " 'earlier': array([ 0.9913342 , -0.13136398]),\n",
       " 'early': array([ 0.94572016, -0.32498213]),\n",
       " 'earnings': array([0.98200811, 0.18883874]),\n",
       " 'eastern': array([ 0.99277591, -0.11998328]),\n",
       " 'easy': array([ 0.96164687, -0.27429055]),\n",
       " 'eating': array([ 0.74062141, -0.67192256]),\n",
       " 'economic': array([0.82267982, 0.56850498]),\n",
       " 'edition': array([0.60702525, 0.79468254]),\n",
       " 'effect': array([ 0.99721849, -0.07453376]),\n",
       " 'effects': array([ 0.75115714, -0.66012344]),\n",
       " 'effort': array([0.80407875, 0.5945228 ]),\n",
       " 'egypt': array([0.99920861, 0.03977619]),\n",
       " 'eight': array([0.99911191, 0.04213551]),\n",
       " 'either': array([ 0.97230463, -0.23371714]),\n",
       " 'el': array([0.98748685, 0.15770135]),\n",
       " 'electric': array([0.81795232, 0.57528602]),\n",
       " 'electronic': array([0.99377522, 0.11140385]),\n",
       " 'elevated': array([0.84062321, 0.54162036]),\n",
       " 'eligible': array([ 0.99783069, -0.06583249]),\n",
       " 'embassies': array([0.74102587, 0.67147648]),\n",
       " 'embassy': array([ 0.97031868, -0.2418298 ]),\n",
       " 'end': array([ 0.9287842 , -0.37062098]),\n",
       " 'ended': array([ 0.98337243, -0.18160028]),\n",
       " 'engineering': array([ 0.99998935, -0.0046158 ]),\n",
       " 'enquiry': array([0.99990856, 0.01352313]),\n",
       " 'entitlements': array([ 0.86030335, -0.50978244]),\n",
       " 'equilibrium': array([ 0.92796576, -0.37266546]),\n",
       " 'equipment': array([0.99553256, 0.09441883]),\n",
       " 'estimate': array([0.95623398, 0.29260309]),\n",
       " 'estimated': array([0.99141468, 0.13075527]),\n",
       " 'estimates': array([ 0.89696665, -0.44209822]),\n",
       " 'evacuated': array([0.86275578, 0.50562087]),\n",
       " 'evacuation': array([ 0.94880376, -0.31586616]),\n",
       " 'even': array([ 0.98312227, -0.18294971]),\n",
       " 'event': array([ 0.99695292, -0.0780056 ]),\n",
       " 'excess': array([0.75271433, 0.65834728]),\n",
       " 'exchange': array([0.98534713, 0.17056094]),\n",
       " 'exchanges': array([ 0.73692391, -0.6759757 ]),\n",
       " 'exclude': array([ 0.73958731, -0.67306063]),\n",
       " 'excluded': array([ 0.99998636, -0.00522346]),\n",
       " 'expected': array([0.97110137, 0.23866739]),\n",
       " 'expedite': array([ 0.89141243, -0.45319299]),\n",
       " 'experienced': array([0.7778736 , 0.62842077]),\n",
       " 'export': array([ 0.99652424, -0.08330333]),\n",
       " 'exporters': array([0.92387761, 0.38268808]),\n",
       " 'exports': array([0.9861471 , 0.16587316]),\n",
       " 'extent': array([ 0.94459146, -0.32824833]),\n",
       " 'fabricated': array([ 0.99886572, -0.04761581]),\n",
       " 'face': array([0.84390568, 0.53649157]),\n",
       " 'faced': array([ 0.75435195, -0.65647021]),\n",
       " 'factors': array([ 0.96422296, -0.26509258]),\n",
       " 'fall': array([0.9988303 , 0.04835315]),\n",
       " 'falling': array([ 0.99097879, -0.13401878]),\n",
       " 'fao': array([ 0.95254043, -0.3044121 ]),\n",
       " 'far': array([0.74027175, 0.67230777]),\n",
       " 'farmers': array([0.98168258, 0.1905238 ]),\n",
       " 'farming': array([0.9999992 , 0.00126697]),\n",
       " 'farms': array([0.91690165, 0.39911322]),\n",
       " 'fear': array([ 0.80736269, -0.5900555 ]),\n",
       " 'february': array([0.99870155, 0.05094325]),\n",
       " 'fertile': array([ 0.99760131, -0.06922156]),\n",
       " 'fibre': array([0.87573546, 0.48279126]),\n",
       " 'figure': array([ 0.86939127, -0.49412429]),\n",
       " 'figures': array([ 0.84832956, -0.52946855]),\n",
       " 'financial': array([ 0.91083308, -0.41277487]),\n",
       " 'find': array([0.7560482 , 0.65451594]),\n",
       " 'firm': array([0.98783731, 0.15549102]),\n",
       " 'firms': array([ 0.9791409 , -0.20318244]),\n",
       " 'fiscal': array([0.96965835, 0.24446406]),\n",
       " 'five': array([0.913067  , 0.40780958]),\n",
       " 'flexible': array([ 0.97204055, -0.23481304]),\n",
       " 'flood': array([ 0.99691769, -0.07845457]),\n",
       " 'floods': array([0.82387805, 0.56676711]),\n",
       " 'food': array([0.94808825, 0.31800733]),\n",
       " 'for': array([ 0.99595381, -0.08986662]),\n",
       " 'forced': array([ 0.84088296, -0.54121701]),\n",
       " 'forecast': array([0.98215668, 0.18806451]),\n",
       " 'forecasting': array([0.94796556, 0.31837289]),\n",
       " 'forecasts': array([ 0.99260376, -0.12139927]),\n",
       " 'foreign': array([0.95511678, 0.29622955]),\n",
       " 'formed': array([ 0.96963693, -0.24454901]),\n",
       " 'found': array([0.88025222, 0.47450609]),\n",
       " 'free': array([0.83107017, 0.55616758]),\n",
       " 'freight': array([0.97278133, 0.23172503]),\n",
       " 'frequently': array([0.88211499, 0.47103412]),\n",
       " 'from': array([0.95414169, 0.29935538]),\n",
       " 'fruit': array([ 0.99891302, -0.04661307]),\n",
       " 'further': array([ 0.95166325, -0.3071434 ]),\n",
       " 'future': array([ 9.99999864e-01, -5.21709649e-04]),\n",
       " 'gain': array([ 0.74482078, -0.66726457]),\n",
       " 'gains': array([0.94227593, 0.33483737]),\n",
       " 'gap': array([0.91143801, 0.41143743]),\n",
       " 'gardens': array([0.90302586, 0.4295862 ]),\n",
       " 'garments': array([ 0.91348583, -0.40687055]),\n",
       " 'gas': array([ 0.97011712, -0.24263714]),\n",
       " 'gatt': array([ 0.99999721, -0.00236306]),\n",
       " 'gave': array([ 0.83451282, -0.55098852]),\n",
       " 'gems': array([ 0.99777622, -0.06665297]),\n",
       " 'general': array([ 0.93829852, -0.34582638]),\n",
       " 'geographically': array([0.7697758 , 0.63831436]),\n",
       " 'georgia': array([ 0.94584658, -0.324614  ]),\n",
       " 'georgian': array([ 0.99547601, -0.09501321]),\n",
       " 'german': array([ 0.99818366, -0.06024434]),\n",
       " 'get': array([ 0.95676198, -0.290872  ]),\n",
       " 'give': array([ 0.99791198, -0.06458853]),\n",
       " 'given': array([0.93413535, 0.35691897]),\n",
       " 'global': array([ 0.94909179, -0.31499965]),\n",
       " 'goods': array([0.98727116, 0.1590461 ]),\n",
       " 'government': array([ 0.88836671, -0.45913461]),\n",
       " 'green': array([0.88589627, 0.46388339]),\n",
       " 'groups': array([ 0.99618993, -0.08721025]),\n",
       " 'groves': array([ 0.97426743, -0.22539513]),\n",
       " 'grow': array([0.94472377, 0.32786736]),\n",
       " 'growers': array([0.95779425, 0.28745466]),\n",
       " 'growing': array([0.84529811, 0.53429497]),\n",
       " 'growth': array([ 0.98903517, -0.14768018]),\n",
       " 'had': array([0.99995685, 0.00928984]),\n",
       " 'half': array([ 0.7724381 , -0.63509006]),\n",
       " 'haq': array([0.7957853 , 0.60557886]),\n",
       " 'harbin': array([0.99469049, 0.1029118 ]),\n",
       " 'hard': array([0.93650457, 0.35065536]),\n",
       " 'harvests': array([0.81089468, 0.58519211]),\n",
       " 'has': array([0.99976915, 0.0214861 ]),\n",
       " 'have': array([ 0.99934489, -0.03619094]),\n",
       " 'hazelnuts': array([ 0.97630228, -0.21641132]),\n",
       " 'he': array([ 0.9978019 , -0.06626747]),\n",
       " 'health': array([0.77814837, 0.6280805 ]),\n",
       " 'hearing': array([ 0.84048738, -0.54183111]),\n",
       " 'heat': array([0.85886838, 0.51219635]),\n",
       " 'heavy': array([ 0.99719013, -0.07491227]),\n",
       " 'hectares': array([0.77164341, 0.63605539]),\n",
       " 'heightened': array([ 0.96817749, -0.25026455]),\n",
       " 'held': array([ 0.74836457, -0.66328762]),\n",
       " 'help': array([0.9927084 , 0.12054063]),\n",
       " 'helping': array([0.84549436, 0.53398436]),\n",
       " 'henan': array([0.90752741, 0.41999286]),\n",
       " 'here': array([ 0.99961653, -0.02769099]),\n",
       " 'high': array([ 0.99639295, -0.0848592 ]),\n",
       " 'higher': array([ 0.90141801, -0.43294986]),\n",
       " 'highlands': array([0.99748662, 0.07085509]),\n",
       " 'hit': array([ 0.81013026, -0.58624992]),\n",
       " 'holders': array([0.79579101, 0.60557136]),\n",
       " 'honey': array([0.87741947, 0.47972396]),\n",
       " 'hong': array([ 0.98751336, -0.15753527]),\n",
       " 'hoppers': array([0.77534968, 0.63153217]),\n",
       " 'house': array([ 0.99991119, -0.01332693]),\n",
       " 'households': array([ 0.99799902, -0.06322933]),\n",
       " 'houses': array([0.95371431, 0.30071418]),\n",
       " 'however': array([ 0.9999757 , -0.00697112]),\n",
       " 'hundreds': array([0.82016987, 0.57212007]),\n",
       " 'hurt': array([0.98880402, 0.14922003]),\n",
       " 'husain': array([ 0.95793187, -0.28699569]),\n",
       " 'husks': array([ 0.99744219, -0.07147783]),\n",
       " 'hydro': array([0.82388808, 0.56675253]),\n",
       " 'if': array([ 0.8975081 , -0.44099798]),\n",
       " 'illustrating': array([ 0.8225532 , -0.56868817]),\n",
       " 'imbalance': array([ 0.89200436, -0.4520268 ]),\n",
       " 'impetus': array([ 0.95835741, -0.2855715 ]),\n",
       " 'import': array([ 0.9938148 , -0.11105021]),\n",
       " 'importer': array([ 0.96808509, -0.25062176]),\n",
       " 'importers': array([0.99379151, 0.11125841]),\n",
       " 'imports': array([0.99367181, 0.11232242]),\n",
       " 'improve': array([ 0.91604583, -0.40107361]),\n",
       " 'in': array([ 0.96566936, -0.25977431]),\n",
       " 'inc': array([0.81440675, 0.58029444]),\n",
       " 'include': array([0.98664158, 0.16290611]),\n",
       " 'including': array([ 0.99814132, -0.06094185]),\n",
       " 'incorrect': array([ 0.99749291, -0.07076652]),\n",
       " 'increase': array([ 0.97017276, -0.24241454]),\n",
       " 'increased': array([0.83883878, 0.54437992]),\n",
       " 'increases': array([0.82890404, 0.55939083]),\n",
       " 'india': array([0.95345049, 0.3015496 ]),\n",
       " 'indian': array([0.95148947, 0.30768132]),\n",
       " 'indonesia': array([ 0.99597827, -0.08959512]),\n",
       " 'indonesian': array([ 0.902153  , -0.43141623]),\n",
       " 'industries': array([0.71804703, 0.69599458]),\n",
       " 'industry': array([ 0.95321028, -0.30230806]),\n",
       " 'inherent': array([0.99979273, 0.0203591 ]),\n",
       " 'inquiry': array([ 0.99992196, -0.01249298]),\n",
       " 'insect': array([ 0.94189674, -0.33590256]),\n",
       " 'insecticides': array([ 0.95376672, -0.3005479 ]),\n",
       " 'insignificant': array([0.80269809, 0.59638559]),\n",
       " 'interested': array([0.8013637 , 0.59817742]),\n",
       " 'interests': array([0.87974161, 0.47545211]),\n",
       " 'international': array([ 0.94743143, -0.31995889]),\n",
       " 'into': array([ 0.96405231, -0.2657125 ]),\n",
       " 'introduction': array([ 0.94427757, -0.32915022]),\n",
       " 'inundated': array([ 0.9984089 , -0.05638861]),\n",
       " 'invest': array([0.76353279, 0.64576905]),\n",
       " 'investigation': array([ 0.98900797, -0.1478622 ]),\n",
       " 'involving': array([ 0.96783806, -0.25157403]),\n",
       " 'iron': array([0.97880605, 0.20478943]),\n",
       " 'irtiza': array([0.83586637, 0.54893297]),\n",
       " 'is': array([ 0.99739593, -0.07212049]),\n",
       " 'it': array([ 0.98583805, -0.16770012]),\n",
       " 'its': array([ 0.9943139 , -0.10648882]),\n",
       " 'izvestia': array([0.93079882, 0.36553187]),\n",
       " 'january': array([0.99938566, 0.03504721]),\n",
       " 'jobs': array([0.88829487, 0.45927359]),\n",
       " 'joerg': array([0.52706672, 0.8498239 ]),\n",
       " 'july': array([0.77725227, 0.62918909]),\n",
       " 'june': array([0.99106263, 0.13339741]),\n",
       " 'just': array([0.82728517, 0.56178221]),\n",
       " 'jute': array([ 0.99429243, -0.10668909]),\n",
       " 'kastl': array([0.68175801, 0.73157776]),\n",
       " 'kenya': array([0.99361848, 0.11279324]),\n",
       " 'kenyan': array([ 0.99563414, -0.09334165]),\n",
       " 'kernels': array([0.9720359 , 0.23483231]),\n",
       " 'key': array([0.99999193, 0.0040167 ]),\n",
       " 'killed': array([ 0.9931105 , -0.11718164]),\n",
       " 'kilo': array([0.99996795, 0.00800598]),\n",
       " 'kilograms': array([0.91324634, 0.40740782]),\n",
       " 'kilometres': array([0.8802934 , 0.47442969]),\n",
       " 'kind': array([0.91126251, 0.41182598]),\n",
       " 'known': array([0.89164132, 0.45274248]),\n",
       " 'kong': array([ 0.99444723, -0.10523647]),\n",
       " 'korean': array([0.99108733, 0.13321375]),\n",
       " 'koreans': array([ 0.94791083, -0.31853579]),\n",
       " 'laboratory': array([0.78611161, 0.61808457]),\n",
       " 'land': array([0.98838726, 0.15195599]),\n",
       " 'lanka': array([0.94567284, 0.32511978]),\n",
       " 'lankan': array([ 0.96381724, -0.26656392]),\n",
       " 'large': array([ 0.88978605, -0.45637791]),\n",
       " 'largely': array([0.85907251, 0.5118539 ]),\n",
       " 'largest': array([ 0.98574211, -0.16826318]),\n",
       " 'last': array([0.99940032, 0.03462646]),\n",
       " 'late': array([0.99783318, 0.06579466]),\n",
       " 'latest': array([ 0.94213018, -0.33524727]),\n",
       " 'launch': array([ 0.95612981, -0.29294331]),\n",
       " 'launched': array([0.82485654, 0.5653421 ]),\n",
       " 'law': array([0.96541102, 0.26073273]),\n",
       " 'least': array([0.81883819, 0.5740244 ]),\n",
       " 'leather': array([ 0.99510445, -0.09882877]),\n",
       " 'leaves': array([ 0.98644228, -0.16410859]),\n",
       " 'less': array([ 0.98555172, -0.16937474]),\n",
       " 'lesser': array([ 0.92391876, -0.38258872]),\n",
       " 'letter': array([ 0.96673714, -0.25577199]),\n",
       " 'level': array([ 0.80677492, -0.59085889]),\n",
       " 'levels': array([0.96376375, 0.26675727]),\n",
       " 'licences': array([ 0.95204785, -0.30594917]),\n",
       " 'likely': array([0.97350632, 0.22866012]),\n",
       " 'limited': array([0.68110871, 0.7321823 ]),\n",
       " 'limits': array([ 0.75943954, -0.65057789]),\n",
       " 'lines': array([0.97897726, 0.20396942]),\n",
       " 'link': array([0.83984857, 0.54282076]),\n",
       " 'linked': array([0.82748426, 0.56148891]),\n",
       " 'linking': array([ 0.7388631 , -0.67385556]),\n",
       " 'links': array([ 0.98784834, -0.15542092]),\n",
       " 'liptons': array([0.9526024 , 0.30421814]),\n",
       " 'little': array([ 0.90486658, -0.42569529]),\n",
       " 'local': array([0.79891261, 0.60144713]),\n",
       " 'long': array([ 0.93977472, -0.34179448]),\n",
       " 'longer': array([0.83781386, 0.54595599]),\n",
       " 'look': array([0.94790566, 0.31855118]),\n",
       " 'losses': array([0.9359012, 0.3522626]),\n",
       " 'low': array([ 0.78633098, -0.61780546]),\n",
       " 'lower': array([0.97493944, 0.22247041]),\n",
       " 'lowered': array([ 0.97513179, -0.2216258 ]),\n",
       " 'lowest': array([ 0.93964   , -0.34216469]),\n",
       " 'lt': array([ 0.89843712, -0.43910219]),\n",
       " 'machinery': array([0.94594612, 0.3243238 ]),\n",
       " 'made': array([ 0.97089593, -0.23950176]),\n",
       " 'mahbubul': array([0.81868425, 0.57424394]),\n",
       " 'main': array([ 0.95003189, -0.31215286]),\n",
       " 'mainly': array([ 0.99785764, -0.06542271]),\n",
       " 'major': array([ 0.96921231, -0.24622653]),\n",
       " 'make': array([ 0.9156353 , -0.40200996]),\n",
       " 'manufactured': array([0.85115918, 0.52490766]),\n",
       " 'many': array([0.98164862, 0.19069866]),\n",
       " 'march': array([0.99814663, 0.06085477]),\n",
       " 'market': array([0.99996649, 0.00818635]),\n",
       " 'markets': array([0.99952096, 0.03094929]),\n",
       " 'matter': array([ 0.97008854, -0.24275137]),\n",
       " 'maximum': array([ 0.76157158, -0.6480808 ]),\n",
       " 'may': array([ 0.99768377, -0.06802271]),\n",
       " 'meal': array([0.92354017, 0.3835017 ]),\n",
       " 'measure': array([ 0.99889307, -0.04703873]),\n",
       " 'measures': array([ 0.88198137, -0.47128428]),\n",
       " 'media': array([ 0.94848426, -0.31682424]),\n",
       " 'meet': array([ 0.940218  , -0.34057322]),\n",
       " 'member': array([ 0.94726181, -0.32046069]),\n",
       " 'members': array([0.83197258, 0.55481674]),\n",
       " 'memory': array([ 0.89374786, -0.44856968]),\n",
       " 'metals': array([ 0.81889851, -0.57393836]),\n",
       " 'meteorological': array([0.80582819, 0.59214942]),\n",
       " 'meteorologists': array([ 0.99952481, -0.03082448]),\n",
       " 'mica': array([ 0.78841449, -0.61514437]),\n",
       " 'mid': array([ 0.98634916, -0.16466734]),\n",
       " 'milk': array([ 0.98009737, -0.19851737]),\n",
       " 'million': array([ 0.9464138 , -0.32295652]),\n",
       " 'mineral': array([0.86026979, 0.50983908]),\n",
       " 'minerals': array([ 0.96923061, -0.24615448]),\n",
       " 'mingles': array([ 0.99375288, -0.11160291]),\n",
       " 'minimum': array([ 0.98948086, -0.14466387]),\n",
       " 'minister': array([0.80854099, 0.58843986]),\n",
       " 'ministry': array([ 0.94523359, -0.32639463]),\n",
       " 'mln': array([0.96463758, 0.26357986]),\n",
       " 'mmtc': array([ 0.91281453, -0.40837437]),\n",
       " 'modest': array([ 0.78029097, -0.62541666]),\n",
       " 'mombasa': array([ 0.86362748, -0.50413052]),\n",
       " 'monday': array([ 0.97579279, -0.21869713]),\n",
       " 'monitoring': array([ 0.99998322, -0.00579224]),\n",
       " 'months': array([ 0.82945929, -0.55856718]),\n",
       " 'more': array([0.98216419, 0.18802527]),\n",
       " 'moscow': array([0.79747378, 0.6033536 ]),\n",
       " 'most': array([ 0.99350964, -0.11374793]),\n",
       " 'mountain': array([ 0.89279995, -0.45045338]),\n",
       " 'move': array([ 0.93390663, -0.35751701]),\n",
       " 'much': array([ 0.75094148, -0.66036876]),\n",
       " 'nairobi': array([ 0.95023665, -0.311529  ]),\n",
       " 'narrow': array([ 0.92641913, -0.37649383]),\n",
       " 'nations': array([ 0.82607472, -0.5635606 ]),\n",
       " 'natural': array([0.88948781, 0.4569589 ]),\n",
       " 'need': array([0.87195204, 0.4895913 ]),\n",
       " 'negative': array([ 0.74598544, -0.66596226]),\n",
       " 'netherlands': array([ 0.97787221, -0.20920309]),\n",
       " 'new': array([ 0.9996019 , -0.02821416]),\n",
       " 'news': array([ 0.83058353, -0.55689407]),\n",
       " 'newspaper': array([ 0.95979651, -0.28069673]),\n",
       " 'nhan': array([ 0.93639918, -0.35093671]),\n",
       " 'nine': array([ 0.90307638, -0.42947998]),\n",
       " 'nino': array([0.99532688, 0.09656299]),\n",
       " 'no': array([ 0.98983705, -0.14220623]),\n",
       " 'non': array([0.99933059, 0.03658387]),\n",
       " 'normal': array([ 0.95160127, -0.30733535]),\n",
       " 'normally': array([ 0.70454218, -0.70966211]),\n",
       " 'northeast': array([0.99459182, 0.10386106]),\n",
       " 'not': array([0.99999365, 0.00356289]),\n",
       " 'noted': array([ 0.97471096, -0.22346934]),\n",
       " 'nothing': array([0.75242369, 0.65867943]),\n",
       " 'now': array([ 0.92513295, -0.37964329]),\n",
       " 'nuclear': array([ 0.72304372, -0.69080227]),\n",
       " 'occur': array([0.99950271, 0.03153304]),\n",
       " 'occurrence': array([ 0.88129207, -0.47257198]),\n",
       " 'ocean': array([ 0.68951169, -0.72427455]),\n",
       " 'oct': array([0.84743174, 0.53090437]),\n",
       " 'october': array([0.84309453, 0.53776539]),\n",
       " 'of': array([ 0.94754839, -0.31961235]),\n",
       " 'off': array([ 0.88629246, -0.46312598]),\n",
       " 'offer': array([0.90003077, 0.43582635]),\n",
       " 'offered': array([ 0.96159581, -0.27446948]),\n",
       " 'offical': array([ 0.95756331, -0.28822303]),\n",
       " 'official': array([ 0.95068535, -0.31015699]),\n",
       " 'officially': array([ 0.79021958, -0.61282381]),\n",
       " 'officials': array([0.96993795, 0.24335235]),\n",
       " 'offset': array([0.99432453, 0.10638951]),\n",
       " 'often': array([0.89790263, 0.44019412]),\n",
       " 'oil': array([0.99687485, 0.07899704]),\n",
       " 'on': array([0.98646303, 0.1639838 ]),\n",
       " 'one': array([0.97772848, 0.2098738 ]),\n",
       " 'only': array([0.96986035, 0.24366143]),\n",
       " 'opened': array([0.96340205, 0.26806062]),\n",
       " 'opium': array([ 0.99554364, -0.09430193]),\n",
       " 'or': array([ 0.96105597, -0.2763538 ]),\n",
       " 'orange': array([ 0.96059775, -0.27794239]),\n",
       " 'order': array([ 0.99862476, -0.05242697]),\n",
       " 'ordered': array([ 0.99981849, -0.01905213]),\n",
       " 'ore': array([0.98048228, 0.19660749]),\n",
       " 'ores': array([0.77127937, 0.63649677]),\n",
       " 'organisation': array([ 0.88846774, -0.45893907]),\n",
       " 'origin': array([0.90956307, 0.4155659 ]),\n",
       " 'other': array([ 0.97694323, -0.21349924]),\n",
       " 'our': array([0.85804465, 0.51357509]),\n",
       " 'outbreak': array([ 0.97979425, -0.20000809]),\n",
       " 'outlets': array([0.76025512, 0.64962462]),\n",
       " 'outlining': array([ 0.99540132, -0.09579254]),\n",
       " 'output': array([0.99334445, 0.11518162]),\n",
       " 'over': array([ 0.9993876 , -0.03499184]),\n",
       " 'overall': array([0.95878064, 0.28414731]),\n",
       " 'overseas': array([0.91682041, 0.39929981]),\n",
       " 'pacific': array([ 0.81572229, -0.57844373]),\n",
       " 'pack': array([0.77135181, 0.63640898]),\n",
       " 'paid': array([ 0.98107981, -0.19360371]),\n",
       " 'pakistan': array([0.99038143, 0.13836412]),\n",
       " 'pakistani': array([ 0.98186136, -0.18960032]),\n",
       " 'palm': array([0.98061881, 0.19592535]),\n",
       " 'paper': array([ 0.85838966, -0.51299823]),\n",
       " 'part': array([0.9820478 , 0.18863224]),\n",
       " 'particular': array([ 0.80646824, -0.59127741]),\n",
       " 'parties': array([0.84607449, 0.53306468]),\n",
       " 'partners': array([0.91405086, 0.40559959]),\n",
       " 'parts': array([ 0.99841398, -0.05629849]),\n",
       " 'party': array([ 0.85946406, -0.51119618]),\n",
       " 'past': array([ 0.88747565, -0.46085461]),\n",
       " 'pattern': array([0.98246195, 0.18646316]),\n",
       " 'patterns': array([ 0.95520803, -0.29593516]),\n",
       " 'payment': array([0.83919387, 0.54383238]),\n",
       " 'payments': array([ 0.95890705, -0.28372041]),\n",
       " 'pct': array([0.99536644, 0.09615432]),\n",
       " 'peking': array([ 0.97740788, -0.21136185]),\n",
       " 'people': array([ 0.99684448, -0.07937937]),\n",
       " 'pepper': array([ 0.97537155, -0.22056821]),\n",
       " 'performance': array([ 0.98351893, -0.18080518]),\n",
       " 'permit': array([ 0.75830146, -0.65190405]),\n",
       " 'permits': array([0.79227986, 0.61015786]),\n",
       " 'permitted': array([0.98037067, 0.19716327]),\n",
       " 'peru': array([ 0.97099361, -0.23910545]),\n",
       " 'pests': array([0.94634359, 0.32316219]),\n",
       " 'petroleum': array([ 0.99995878, -0.00908001]),\n",
       " 'pharmaceuticals': array([ 0.9945865 , -0.10391192]),\n",
       " 'phenomenon': array([ 0.77538955, -0.63148321]),\n",
       " 'places': array([0.94692263, 0.32146155]),\n",
       " 'planning': array([ 0.97824387, -0.20745827]),\n",
       " 'plans': array([0.91633934, 0.40040257]),\n",
       " 'plant': array([ 0.99439867, -0.10569427]),\n",
       " 'plantation': array([0.79451614, 0.60724303]),\n",
       " 'plantations': array([ 0.96589158, -0.25894682]),\n",
       " 'planters': array([0.87778413, 0.47905639]),\n",
       " 'planting': array([0.80859597, 0.58836431]),\n",
       " 'plants': array([0.8919704, 0.4520938]),\n",
       " 'plastics': array([ 0.98828055, -0.15264845]),\n",
       " 'platform': array([0.97369765, 0.227844  ]),\n",
       " 'plc': array([0.82646367, 0.56299006]),\n",
       " 'plywood': array([0.96010834, 0.27962829]),\n",
       " 'po': array([0.72519239, 0.68854629]),\n",
       " 'point': array([ 0.93329335, -0.3591149 ]),\n",
       " 'policy': array([ 0.96938379, -0.24555054]),\n",
       " 'pork': array([ 0.99290511, -0.11890939]),\n",
       " 'pose': array([ 0.735062  , -0.67799989]),\n",
       " 'possible': array([0.92046579, 0.39082315]),\n",
       " 'power': array([0.8504609 , 0.52603827]),\n",
       " 'prayer': array([ 0.97813224, -0.20798393]),\n",
       " 'precautionary': array([0.95403236, 0.2997036 ]),\n",
       " 'precise': array([ 0.9801496 , -0.19825932]),\n",
       " 'preference': array([0.98837256, 0.15205156]),\n",
       " 'prescribed': array([ 0.91975396, -0.39249542]),\n",
       " 'present': array([ 0.85687985, -0.51551617]),\n",
       " 'press': array([0.97174823, 0.23601987]),\n",
       " 'pressure': array([0.8992837 , 0.43736579]),\n",
       " 'preventive': array([ 0.8707403 , -0.49174316]),\n",
       " 'previous': array([ 0.76421868, -0.64495721]),\n",
       " 'prices': array([ 0.99839452, -0.05664256]),\n",
       " 'private': array([0.97101371, 0.23902381]),\n",
       " 'probable': array([ 0.99616409, -0.08750492]),\n",
       " 'problem': array([ 0.98349521, -0.18093415]),\n",
       " 'problems': array([0.99532445, 0.09658803]),\n",
       " 'procedures': array([0.91746895, 0.3978074 ]),\n",
       " 'processed': array([ 0.99276983, -0.12003362]),\n",
       " 'processing': array([0.89218284, 0.45167441]),\n",
       " 'produce': array([0.99983424, 0.01820674]),\n",
       " 'produced': array([ 0.99924364, -0.03888639]),\n",
       " 'producing': array([ 0.83563588, -0.54928378]),\n",
       " 'product': array([ 0.93763784, -0.3476137 ]),\n",
       " 'production': array([0.94303299, 0.33269924]),\n",
       " 'products': array([ 0.98416927, -0.17723104]),\n",
       " 'projected': array([0.99960161, 0.02822451]),\n",
       " 'promised': array([0.98635373, 0.16464   ]),\n",
       " 'promising': array([0.8926219 , 0.45080611]),\n",
       " 'promote': array([0.96907887, 0.24675118]),\n",
       " 'promoted': array([0.90811307, 0.41872503]),\n",
       " 'promoting': array([0.78115639, 0.6243354 ]),\n",
       " 'promotion': array([ 0.91035091, -0.4138372 ]),\n",
       " 'promotional': array([0.80319202, 0.59572022]),\n",
       " 'pronounced': array([ 0.83023908, -0.55740745]),\n",
       " 'prospect': array([ 0.91332613, -0.40722891]),\n",
       " 'prospects': array([0.99919057, 0.04022688]),\n",
       " 'protectionism': array([0.83063783, 0.55681307]),\n",
       " 'protest': array([0.99871086, 0.05076047]),\n",
       " 'protocol': array([0.92808528, 0.37236771]),\n",
       " 'provide': array([ 0.76934457, -0.63883404]),\n",
       " 'provided': array([0.91308035, 0.4077797 ]),\n",
       " 'provides': array([0.9657422 , 0.25950339]),\n",
       " 'providing': array([0.90727346, 0.42054117]),\n",
       " 'provinces': array([0.96402834, 0.26579946]),\n",
       " 'psyllium': array([ 0.99720057, -0.07477321]),\n",
       " 'pump': array([ 0.78687214, -0.61711607]),\n",
       " 'purchase': array([ 0.94008643, -0.34093622]),\n",
       " 'purposes': array([ 0.999996  , -0.00282766]),\n",
       " 'push': array([ 0.93340845, -0.35881564]),\n",
       " 'put': array([0.88809107, 0.45966755]),\n",
       " 'qualify': array([ 0.91106721, -0.41225786]),\n",
       " 'qualities': array([0.81955021, 0.57300737]),\n",
       " 'quality': array([0.97612839, 0.21719432]),\n",
       " 'queensland': array([0.98429459, 0.17653372]),\n",
       " 'quietly': array([ 0.76220554, -0.64733509]),\n",
       " 'quotas': array([ 0.959606  , -0.28134734]),\n",
       " 'radiation': array([0.77704148, 0.62944939]),\n",
       " 'rails': array([0.99915561, 0.04108621]),\n",
       " 'railway': array([0.9755708 , 0.21968525]),\n",
       " 'rain': array([ 0.86172509, -0.50737546]),\n",
       " 'rainfall': array([0.99861705, 0.05257357]),\n",
       " 'rains': array([ 0.90860595, -0.41765444]),\n",
       " 'rainy': array([ 0.94312568, -0.3324364 ]),\n",
       " 'raise': array([ 0.74749539, -0.664267  ]),\n",
       " 'rate': array([ 0.88706276, -0.46164885]),\n",
       " 'raw': array([ 0.99691973, -0.07842863]),\n",
       " 'rbi': array([0.9103155 , 0.41391507]),\n",
       " 're': array([0.99946518, 0.03270098]),\n",
       " 'reached': array([0.95414446, 0.29934653]),\n",
       " 'reaches': array([ 0.86763195, -0.497207  ]),\n",
       " 'readings': array([ 0.97477998, -0.2231681 ]),\n",
       " 'ready': array([ 0.99112146, -0.13295959]),\n",
       " 'received': array([ 0.98947715, -0.14468919]),\n",
       " 'recent': array([ 0.87294475, -0.48781908]),\n",
       " 'recently': array([0.99275788, 0.12013242]),\n",
       " 'recommended': array([0.99754917, 0.06996888]),\n",
       " 'record': array([0.85594258, 0.51707089]),\n",
       " 'recorded': array([0.60303717, 0.79771309]),\n",
       " 'reduce': array([ 0.94903718, -0.31516413]),\n",
       " 'reduced': array([0.87210377, 0.48932097]),\n",
       " 'region': array([ 0.7695904 , -0.63853787]),\n",
       " 'regulations': array([0.99931672, 0.03696075]),\n",
       " 'reintensification': array([ 0.98887178, -0.1487703 ]),\n",
       " 'relaxes': array([ 0.99943138, -0.03371829]),\n",
       " 'release': array([0.85750752, 0.51447143]),\n",
       " 'relied': array([ 0.92956678, -0.36865378]),\n",
       " 'relocation': array([0.99996468, 0.00840524]),\n",
       " 'reluctance': array([0.9892648 , 0.14613406]),\n",
       " 'remained': array([ 0.98435481, -0.17619763]),\n",
       " 'remaining': array([ 0.91667795, -0.39962675]),\n",
       " 'remains': array([ 0.74543585, -0.66657737]),\n",
       " 'repayment': array([ 0.95148201, -0.30770439]),\n",
       " 'replace': array([9.99999638e-01, 8.51391351e-04]),\n",
       " 'replaces': array([ 0.77068684, -0.63721408]),\n",
       " 'report': array([ 0.95495565, -0.29674857]),\n",
       " 'reported': array([0.95365188, 0.3009121 ]),\n",
       " 'reports': array([ 0.87750617, -0.47956535]),\n",
       " 'republic': array([ 0.99992023, -0.01263083]),\n",
       " 'requested': array([ 0.95703523, -0.28997167]),\n",
       " 'requirement': array([ 0.8696227 , -0.49371688]),\n",
       " 'reserve': array([ 0.94588643, -0.32449787]),\n",
       " 'reserves': array([0.92303056, 0.38472663]),\n",
       " 'resettle': array([0.86506462, 0.50166044]),\n",
       " 'resettled': array([0.641539  , 0.76709042]),\n",
       " 'residents': array([0.83111025, 0.55610768]),\n",
       " 'resistant': array([ 0.93292785, -0.36006337]),\n",
       " 'respect': array([0.85058128, 0.52584359]),\n",
       " 'respective': array([0.89845145, 0.43907288]),\n",
       " 'responsible': array([ 0.96553909, -0.26025807]),\n",
       " 'restrictive': array([0.96421728, 0.26511324]),\n",
       " 'result': array([0.98562995, 0.16891893]),\n",
       " 'resulted': array([0.85646966, 0.51619736]),\n",
       " 'resumed': array([ 0.96358805, -0.26739122]),\n",
       " 'return': array([0.98688797, 0.16140672]),\n",
       " 'returns': array([ 0.79137307, -0.61133352]),\n",
       " 'reuters': array([ 0.98769056, -0.15642045]),\n",
       " 'revenue': array([0.93664687, 0.35027511]),\n",
       " 'rice': array([0.99690692, 0.07859129]),\n",
       " 'rights': array([0.96491841, 0.26254992]),\n",
       " 'rigs': array([0.95791075, 0.2870662 ]),\n",
       " 'rise': array([0.98538603, 0.17033604]),\n",
       " 'rising': array([ 0.99864254, -0.05208727]),\n",
       " 'river': array([ 0.91326985, -0.40735511]),\n",
       " 'rivers': array([0.85365723, 0.52083522]),\n",
       " 'roads': array([0.82678226, 0.56252209]),\n",
       " 'role': array([ 0.99745839, -0.07125137]),\n",
       " 'room': array([0.99606475, 0.08862848]),\n",
       " 'roubles': array([0.87791399, 0.47881836]),\n",
       " 'rubber': array([ 0.97884921, -0.20458306]),\n",
       " 'rule': array([ 0.94322273, -0.33216091]),\n",
       " 'rules': array([ 0.97903819, -0.20367675]),\n",
       " 'run': array([ 0.98405646, -0.17785635]),\n",
       " 'rupees': array([0.99864726, 0.05199669]),\n",
       " 'rupiah': array([ 0.99379673, -0.1112118 ]),\n",
       " 'russia': array([0.8786596 , 0.47744875]),\n",
       " 's': array([0.99929789, 0.03746647]),\n",
       " 'said': array([ 0.87793037, -0.47878834]),\n",
       " 'same': array([ 0.75132991, -0.65992678]),\n",
       " 'samples': array([0.8447012 , 0.53523815]),\n",
       " 'sandalwood': array([ 0.99780599, -0.06620575]),\n",
       " 'saying': array([0.92592746, 0.37770139]),\n",
       " 'says': array([ 0.93015346, -0.36717101]),\n",
       " 'scale': array([ 0.74606331, -0.66587501]),\n",
       " 'schedule': array([ 0.97858778, -0.20582992]),\n",
       " 'scheme': array([ 0.88949355, -0.45694774]),\n",
       " 'scientists': array([0.88345108, 0.46852342]),\n",
       " 'sea': array([ 0.79368289, -0.60833171]),\n",
       " 'searching': array([0.9935422 , 0.11346323]),\n",
       " 'seas': array([ 0.97322811, -0.22984136]),\n",
       " 'season': array([ 0.84477165, -0.53512696]),\n",
       " 'second': array([ 0.96180069, -0.27375068]),\n",
       " 'secondary': array([ 0.83800574, -0.54566141]),\n",
       " 'sector': array([ 0.98675167, -0.16223791]),\n",
       " 'seeds': array([0.9439186 , 0.33017825]),\n",
       " 'seek': array([0.96160481, 0.27443797]),\n",
       " 'seeking': array([ 0.72310311, -0.6907401 ]),\n",
       " 'seen': array([ 0.99181296, -0.12769905]),\n",
       " 'selected': array([ 0.98810039, -0.15381033]),\n",
       " 'sell': array([0.98115317, 0.19323162]),\n",
       " 'selling': array([0.8146564 , 0.57994392]),\n",
       " 'semi': array([ 0.9986713 , -0.05153285]),\n",
       " 'sends': array([ 0.92578191, -0.37805801]),\n",
       " 'sent': array([ 0.89852611, -0.43892008]),\n",
       " 'separate': array([ 0.94907895, -0.31503834]),\n",
       " 'sept': array([0.85773507, 0.51409197]),\n",
       " 'september': array([ 0.95201241, -0.30605943]),\n",
       " 'serious': array([0.95112412, 0.30880885]),\n",
       " 'service': array([0.95563112, 0.29456607]),\n",
       " 'services': array([ 0.95766592, -0.28788191]),\n",
       " 'set': array([ 0.98774768, -0.15605934]),\n",
       " 'seven': array([0.96498195, 0.26231629]),\n",
       " 'several': array([0.89335451, 0.44935256]),\n",
       " 'severe': array([0.88736157, 0.46107423]),\n",
       " 'shift': array([0.96057211, 0.27803096]),\n",
       " 'shifting': array([ 0.79722393, -0.6036837 ]),\n",
       " 'shillings': array([ 0.94575747, -0.32487353]),\n",
       " 'shipping': array([0.88221839, 0.47084043]),\n",
       " 'ships': array([0.94270211, 0.33363562]),\n",
       " 'shops': array([0.91660351, 0.39979746]),\n",
       " 'short': array([ 0.99999503, -0.003152  ]),\n",
       " 'shortage': array([0.9215852 , 0.38817615]),\n",
       " 'shortages': array([0.96342027, 0.26799513]),\n",
       " 'shortly': array([ 0.9784391 , -0.20653553]),\n",
       " 'should': array([ 0.92643879, -0.37644544]),\n",
       " 'show': array([ 0.98355303, -0.18061957]),\n",
       " 'showed': array([ 0.89808743, -0.43981698]),\n",
       " 'showing': array([0.78293254, 0.62210661]),\n",
       " 'shown': array([ 0.99923046, -0.03922356]),\n",
       " 'shrinking': array([0.96724868, 0.25383065]),\n",
       " 'sichuan': array([0.85566711, 0.51752662]),\n",
       " 'signals': array([0.99996354, 0.00853908]),\n",
       " 'signed': array([0.98010732, 0.19846822]),\n",
       " 'significant': array([0.82621796, 0.56335058]),\n",
       " 'significantly': array([ 0.80659965, -0.59109814]),\n",
       " 'since': array([ 0.97328758, -0.22958939]),\n",
       " 'single': array([0.92976752, 0.3681472 ]),\n",
       " 'six': array([ 0.98526481, -0.1710358 ]),\n",
       " 'slight': array([ 0.73256211, -0.68070019]),\n",
       " 'slightly': array([ 0.99963675, -0.02695131]),\n",
       " 'slow': array([ 0.99537622, -0.09605301]),\n",
       " 'slump': array([ 0.79149972, -0.61116953]),\n",
       " 'small': array([ 0.97885891, -0.20453664]),\n",
       " 'snows': array([ 0.96504693, -0.26207713]),\n",
       " 'so': array([0.86237135, 0.50627626]),\n",
       " 'sold': array([0.98507585, 0.17212081]),\n",
       " 'some': array([ 0.99970519, -0.02428045]),\n",
       " 'sought': array([ 0.73477044, -0.67831585]),\n",
       " 'source': array([0.96402016, 0.26582915]),\n",
       " 'sources': array([ 0.98107315, -0.19363749]),\n",
       " 'south': array([ 0.93412261, -0.35695232]),\n",
       " 'southern': array([0.99430088, 0.10661034]),\n",
       " 'soviet': array([0.99390788, 0.11021396]),\n",
       " 'sowing': array([0.7612005 , 0.64851661]),\n",
       " 'soybean': array([ 0.99392862, -0.11002684]),\n",
       " 'soybeans': array([0.86333718, 0.5046275 ]),\n",
       " 'spanish': array([0.70662723, 0.70758601]),\n",
       " 'specialises': array([0.81433167, 0.5803998 ]),\n",
       " 'speed': array([0.9910546 , 0.13345706]),\n",
       " 'spending': array([0.80440053, 0.59408736]),\n",
       " 'spent': array([0.9994464 , 0.03327003]),\n",
       " 'spokesman': array([0.9151546 , 0.40310304]),\n",
       " 'spokeswoman': array([0.90051565, 0.4348236 ]),\n",
       " 'sports': array([ 0.9949821 , -0.10005305]),\n",
       " 'spotlight': array([ 0.74709438, -0.66471797]),\n",
       " 'spring': array([0.71519044, 0.69892963]),\n",
       " 'sri': array([0.99185946, 0.12733739]),\n",
       " 'stage': array([0.95916988, 0.28283058]),\n",
       " 'stages': array([0.84508904, 0.53462559]),\n",
       " 'stagnates': array([ 0.93508297, -0.35442888]),\n",
       " 'staple': array([0.94929595, 0.31438384]),\n",
       " 'start': array([ 0.95647903, -0.29180107]),\n",
       " 'state': array([ 0.98424771, -0.17679494]),\n",
       " 'statement': array([ 0.8987978 , -0.43836344]),\n",
       " 'states': array([ 0.82913749, -0.55904474]),\n",
       " 'stc': array([ 0.82489508, -0.56528586]),\n",
       " 'steadily': array([ 0.95696063, -0.29021778]),\n",
       " 'steel': array([0.96119362, 0.27587467]),\n",
       " 'steps': array([0.99997661, 0.00684025]),\n",
       " 'still': array([ 0.99934395, -0.03621695]),\n",
       " 'stocks': array([0.95700219, 0.29008071]),\n",
       " 'stood': array([0.99495293, 0.10034271]),\n",
       " 'stopped': array([0.99983387, 0.0182274 ]),\n",
       " 'stored': array([0.91572705, 0.40180091]),\n",
       " 'stream': array([ 0.97088014, -0.23956577]),\n",
       " 'strengthened': array([ 0.96188205, -0.27346465]),\n",
       " 'strike': array([ 0.99760744, -0.06913322]),\n",
       " 'striken': array([ 0.98065315, -0.19575343]),\n",
       " 'strip': array([0.8143672 , 0.58034995]),\n",
       " 'structures': array([0.963065  , 0.26926902]),\n",
       " 'studied': array([0.73600812, 0.67697271]),\n",
       " 'such': array([0.93782746, 0.34710179]),\n",
       " 'sudden': array([ 0.99646966, -0.0839537 ]),\n",
       " 'sugarcane': array([ 0.99737581, -0.07239811]),\n",
       " 'sum': array([0.86772886, 0.49703786]),\n",
       " 'supply': array([0.9995063 , 0.03141894]),\n",
       " 'support': array([ 0.7472278 , -0.66456799]),\n",
       " 'surge': array([0.86843524, 0.49580261]),\n",
       " 'surprise': array([ 0.99945578, -0.03298694]),\n",
       " 'suspend': array([0.97403221, 0.2264095 ]),\n",
       " 'suspended': array([ 0.94090033, -0.33868359]),\n",
       " 'swaps': array([ 0.95430899, -0.2988216 ]),\n",
       " 't': array([0.96250017, 0.27128106]),\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebb48c",
   "metadata": {},
   "source": [
    "Define a function that given a word $w$ identify its most similar words in your embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4575b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:00:15.486508Z",
     "start_time": "2021-10-25T13:00:15.468166Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar(query_word, word_matrix, word_indices, topn=10):\n",
    "    \"\"\"Return the words that have the closest embedding to the queried word.\"\"\"\n",
    "    \n",
    "    ind2word = {value: key for key, value in word_indices.items()}\n",
    "    \n",
    "    query_word_idx = word_indices[query_word]\n",
    "    query_word_embedding = word_matrix[query_word_idx]\n",
    "    \n",
    "    similarities = [(ind2word[i], cosine_similarity(query_word_embedding.reshape(1, -1), row.reshape(1, -1))[0][0]) for i, row in enumerate(word_matrix) if i != query_word_idx]\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7f5948f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:00:17.852371Z",
     "start_time": "2021-10-25T13:00:17.698920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('congress', 0.9999999917994921), ('enquiry', 0.9999996608568608),\n",
      " ('dramatic', 0.9999993214106657), ('any', 0.9999991556091371),\n",
      " ('citizenship', 0.9999988594142523), ('cocoa', 0.9999984966604832),\n",
      " ('and', 0.9999933066559741), ('produce', 0.999992547807904),\n",
      " ('stopped', 0.9999924678157199), ('had', 0.9999872126469822)]\n"
     ]
    }
   ],
   "source": [
    "pp(most_similar(\"tea\", co_matrix_normalized, word_idx), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6b916",
   "metadata": {},
   "source": [
    "The above similarities do not seem really convincing. Hence, the corpus we have been using so far only contains 13 documents, which is way to small to create great embeddings. For good results, we should instead use at least 100k different documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d6f11",
   "metadata": {},
   "source": [
    "You can try it out yourself with another corpus of text without any problem -- all those functions are totally reusable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75eee47",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f0c9d",
   "metadata": {},
   "source": [
    "Now, let's have a look at a model that has already been pretrained on millions of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aff5f5",
   "metadata": {},
   "source": [
    "## GloVe encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7077ed",
   "metadata": {},
   "source": [
    "Word2Vec models are predictive by essence, since it is a neural network. However, this is not the sole method to learn geometrical encodings (vectors) of words from their co-occurrence information (how frequently they appear together in large text corpora).\n",
    "\n",
    "GloVe is a count-based model that learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix. Does it remind you of something? Yes, that's exactly what you did above.\n",
    "\n",
    "Building models is time consuming. Hence, GloVe / Word2Vec models already trained on regular training sets (Wikipedia, News, etc.) are publicly shared to be reused easily.\n",
    "\n",
    "The below code will load a Glove model trained on wikipedia and allow us inspect easily the embeddings properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a872a3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:00:53.166733Z",
     "start_time": "2021-10-25T13:00:52.787854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300',\n",
      " 'conceptnet-numberbatch-17-06-300',\n",
      " 'word2vec-ruscorpora-300',\n",
      " 'word2vec-google-news-300',\n",
      " 'glove-wiki-gigaword-50',\n",
      " 'glove-wiki-gigaword-100',\n",
      " 'glove-wiki-gigaword-200',\n",
      " 'glove-wiki-gigaword-300',\n",
      " 'glove-twitter-25',\n",
      " 'glove-twitter-50',\n",
      " 'glove-twitter-100',\n",
      " 'glove-twitter-200',\n",
      " '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# List all pretrained models available on gensim\n",
    "pp(list(gensim_api.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72fb7437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:08.055789Z",
     "start_time": "2021-10-25T13:00:56.380241Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim_api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae648da",
   "metadata": {},
   "source": [
    "You can get the most similar embeddings to those of a given set of words. \n",
    "Here, we retrieve the most similar words to fox, rabbit and cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "254dd7b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:08.384065Z",
     "start_time": "2021-10-25T13:01:08.308217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 0.8569531440734863), ('mouse', 0.7859790921211243),\n",
      " ('monster', 0.7710846066474915), ('wolf', 0.7690606713294983),\n",
      " ('bunny', 0.765525221824646), ('spider', 0.7395666241645813),\n",
      " ('duck', 0.7366620898246765), ('rat', 0.7366542220115662),\n",
      " ('beast', 0.7319128513336182), ('cartoon', 0.724099338054657)]\n"
     ]
    }
   ],
   "source": [
    "pp(model.most_similar(positive=[\"fox\", \"rabbit\", \"cat\"]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b5d39",
   "metadata": {},
   "source": [
    "You can also perform concept additions / soustractions. For instance, you can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af07844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:08.662589Z",
     "start_time": "2021-10-25T13:01:08.634038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523604273796082),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.7592144012451172),\n",
       " ('daughter', 0.7473883628845215),\n",
       " ('elizabeth', 0.7460219860076904),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337412238121033),\n",
       " ('monarch', 0.721449077129364),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099431157112122)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba43229",
   "metadata": {},
   "source": [
    "Explore those embeddings and comment on how they help to identify / handle:\n",
    "\n",
    "* Synonyms\n",
    "* Antonyms\n",
    "* Grammatical errors\n",
    "* Polysemy\n",
    "* *~~[Irony]~~ -> this was removed and should be instead done on sentence embeddings*\n",
    "* *~~[Analogies]~~ -> this was removed and should be instead done on sentence embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ad7c11f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:14.979423Z",
     "start_time": "2021-10-25T13:01:14.972345Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_analogies(model, words):\n",
    "    for word in words:\n",
    "        results = model.most_similar(positive=[word], topn=10)\n",
    "        results = [f'{r[0]} (*{np.round(r[1], 2)}*)' for r in results]\n",
    "        display(Markdown(f\"Words that are most similar to <b>{word}</b> are:\"))\n",
    "        display(Markdown(f\"* {', '.join(results)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13052946",
   "metadata": {},
   "source": [
    "### Synonyms & antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025083e",
   "metadata": {},
   "source": [
    "First, let's have a look at simple nouns to see if we find their synonyms within their most similar embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee187418",
   "metadata": {},
   "source": [
    "#### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70035626",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:16.269628Z",
     "start_time": "2021-10-25T13:01:16.193664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>petrol</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* gasoline (*0.84*), diesel (*0.79*), kerosene (*0.78*), lpg (*0.77*), fuel (*0.75*), propane (*0.75*), liter (*0.74*), litres (*0.7*), litre (*0.7*), liters (*0.7*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>meal</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* meals (*0.86*), snack (*0.78*), bread (*0.78*), eat (*0.77*), lunch (*0.77*), breakfast (*0.77*), ate (*0.76*), eating (*0.76*), dessert (*0.75*), dinner (*0.74*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>data</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* information (*0.83*), tracking (*0.81*), database (*0.81*), analysis (*0.8*), applications (*0.79*), indicate (*0.77*), indicates (*0.76*), computer (*0.76*), indicating (*0.76*), user (*0.76*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>cat</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* dog (*0.92*), rabbit (*0.85*), monkey (*0.8*), rat (*0.79*), cats (*0.79*), snake (*0.78*), dogs (*0.78*), pet (*0.78*), mouse (*0.77*), bite (*0.77*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_analogies(model, [\"petrol\", \"meal\", \"data\", \"cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c2e77",
   "metadata": {},
   "source": [
    "It appears here that for nouns, the similarity are working pretty great: the highest linked words are mostly synonyms and the following seem to be related ideas.\n",
    "\n",
    "We can also note that some verbes are returned as well: *meal* -> *eat*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d9795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T10:22:29.048862Z",
     "start_time": "2021-10-25T10:22:29.040769Z"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32497c67",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a34b7",
   "metadata": {},
   "source": [
    "Now, let's look at some adjectives to see if we can observe the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1619b44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:18.173324Z",
     "start_time": "2021-10-25T13:01:18.112866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>awesome</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* unbelievable (*0.86*), amazing (*0.86*), incredible (*0.85*), fantastic (*0.81*), marvelous (*0.79*), terrific (*0.78*), phenomenal (*0.74*), truly (*0.74*), luck (*0.72*), damn (*0.71*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>great</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* greatest (*0.82*), good (*0.8*), perhaps (*0.79*), life (*0.78*), well (*0.78*), little (*0.78*), much (*0.78*), inspiration (*0.78*), luck (*0.78*), experience (*0.77*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>awful</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* horrible (*0.93*), terrible (*0.89*), dreadful (*0.87*), unbelievable (*0.85*), scary (*0.83*), weird (*0.82*), sadly (*0.81*), sad (*0.81*), frightening (*0.81*), thing (*0.8*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_analogies(model, [\"awesome\", \"great\", \"awful\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b663576",
   "metadata": {},
   "source": [
    "We can observe here that adjectives relationship are almost as good as for the nouns, but that:\n",
    "\n",
    "* There are sometimes some opposite ideas that have a high similarity (*awesome* -> *terrific*)\n",
    "* It doesn't necessarily output only adjectives (*great* -> *life*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4c181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T10:33:29.638074Z",
     "start_time": "2021-10-25T10:33:29.628102Z"
    }
   },
   "source": [
    "### Grammatical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e99a546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:19.288976Z",
     "start_time": "2021-10-25T13:01:18.960622Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'awsome' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MAXIME~1.SAZ\\AppData\\Local\\Temp/ipykernel_19208/3553936933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay_analogies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"awsome\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\MAXIME~1.SAZ\\AppData\\Local\\Temp/ipykernel_19208/894361936.py\u001b[0m in \u001b[0;36mdisplay_analogies\u001b[1;34m(model, words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_analogies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf'{r[0]} (*{np.round(r[1], 2)}*)'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Words that are most similar to <b>{word}</b> are:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_envs\\Course_ESIEA\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_envs\\Course_ESIEA\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python_envs\\Course_ESIEA\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'awsome' not present\""
     ]
    }
   ],
   "source": [
    "display_analogies(model, [\"awsome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27bf23ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:23.405316Z",
     "start_time": "2021-10-25T13:01:23.372798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>helo</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* alcmene (*0.79*), vondas (*0.75*), kurama (*0.74*), selvi (*0.74*), annu (*0.74*), gorath (*0.72*), drey (*0.72*), siya (*0.72*), malli (*0.72*), nanu (*0.72*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_analogies(model, [\"helo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d61b6aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:19.505711Z",
     "start_time": "2021-10-25T13:01:19.480465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>maket</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* +8.00 (*0.8*), gaint (*0.79*), +7.50 (*0.78*), +11.00 (*0.77*), telecommunciations (*0.76*), -9.00 (*0.76*), +1.75 (*0.76*), +1.25 (*0.76*), +7.00 (*0.76*), -6.50 (*0.74*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_analogies(model, [\"maket\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d7e412",
   "metadata": {},
   "source": [
    "We can see here that grammatical errors are a real problem for such embeddings. If they're not found in the dictionnary, you will have to dismiss them. However, for *maket* and *helo*, another problem occurs: they are either related to another concept or another language, and this could lead to big problems when trying to build downstream classification systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420173bd",
   "metadata": {},
   "source": [
    "How can we solve that? By modifying the **tokenizer** that we are using.\n",
    "\n",
    "A tokenizer is in charge of preparing the inputs for a model, i.e. of splitting the text in relevant tokens. For instance, tokenizers can represent *reading* as two tokens, helping the system in understanding what is the root word and what is its conjugation:\n",
    "* reading -> *[\"read\", \"-ing\"]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d923888",
   "metadata": {},
   "source": [
    "If you want to know more about tokenizers, you can head here: [Tokenizer introduction](https://huggingface.co/transformers/tokenizer_summary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15e532",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67568a28",
   "metadata": {},
   "source": [
    "### Polysemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ebb12",
   "metadata": {},
   "source": [
    "Let's have a closer look at an obvious polysemy example and check how the model handles it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bde4a119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:27.653513Z",
     "start_time": "2021-10-25T13:01:27.625698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Words that are most similar to <b>mouse</b> are:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* monkey (*0.8*), bugs (*0.78*), cat (*0.77*), rabbit (*0.76*), worm (*0.75*), clone (*0.73*), robot (*0.73*), spider (*0.72*), bug (*0.71*), frog (*0.7*)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_analogies(model, [\"mouse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c3ba7",
   "metadata": {},
   "source": [
    "We can see here that the mouse used alongside your computer has been totally left out. \n",
    "\n",
    "If we were able to indicate that we want to obtain the word as a peripheral, we might be able to search differently in the embedding space. You see where I'm heading, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e67f6a",
   "metadata": {},
   "source": [
    "When searching for the vector that is the most similar something, we can define that something as a word (that is what we have been doing so far) or as a concept $(king - man + woman)$.\n",
    "\n",
    "Let's try it out for mouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a30d2800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:29.882089Z",
     "start_time": "2021-10-25T13:01:29.852193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brain', 0.7404634356498718),\n",
       " ('neural', 0.7243039608001709),\n",
       " ('brains', 0.7190447449684143),\n",
       " ('neurons', 0.7073276042938232),\n",
       " ('interface', 0.7037647366523743),\n",
       " ('uses', 0.6876873970031738),\n",
       " ('circuitry', 0.6868135333061218),\n",
       " ('mimic', 0.6858026385307312),\n",
       " ('bugs', 0.6839889287948608),\n",
       " ('devices', 0.6759170889854431)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"mouse\", \"peripheral\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefa98b",
   "metadata": {},
   "source": [
    "The results are okay here (we see bugs, devices), but we also have some elements related to the neurobiology field here. Let's precise that we do not want to work on the animal component of the mouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa559696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:31.638035Z",
     "start_time": "2021-10-25T13:01:31.613175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adapter', 0.715379536151886),\n",
       " ('usb', 0.7070850729942322),\n",
       " ('switches', 0.7027341723442078),\n",
       " ('connectors', 0.7009559273719788),\n",
       " ('plugs', 0.6908823847770691),\n",
       " ('socket', 0.6843112707138062),\n",
       " ('circuitry', 0.6815988421440125),\n",
       " ('neuropathy', 0.6803703308105469),\n",
       " ('sockets', 0.6705275177955627),\n",
       " ('adapters', 0.6666727066040039)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"mouse\", \"peripheral\"], negative=[\"animal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc9f847",
   "metadata": {},
   "source": [
    "It looks better now!\n",
    "\n",
    "When using such embeddings in downstream models, it is hard to do what we've done here since you do not necessarily have a knowledge graph at hand. Instead, consider using context-aware word embeddings such as ELMo or BERT (to be added on top of your model). You can also have a look at [sentence-transformers](https://github.com/UKPLab/sentence-transformers) that output multilingual context-aware embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766015a1",
   "metadata": {},
   "source": [
    "### Recap - Where do you think those behaviours come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae177de0",
   "metadata": {},
   "source": [
    "Since GloVe embeddings are trained based on co-occurence, two words will have a high similarity if they can be used in the **same context**. Hence, if two words are easily interchangeable it means that they should have a high similarity as well. This is the reason why adjectives might have a high similarity with their antonyms.\n",
    "\n",
    "Regarding the polysemy, the most represented concept in the embedding correspond to the most represented concept in the corpus you have trained your model on.\n",
    "\n",
    "Regarding grammatical errors, the sourcing of the training data might cause the problem. When crawling data on the web one might catch other languages by mistake and not detect it easily. Because of that, some grammatical errors are detected as different words with a total different meaning, which can be dangerous in some use cases. Try picking a tokenizer that has been designed to handle grammatical errors if this applies to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9980b8",
   "metadata": {},
   "source": [
    "# Part II - Sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec10d2",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0688c",
   "metadata": {},
   "source": [
    "The TF-IDF is a methodology aiming at finding the most significative words in each document by comparing their in-document frequency to the overall frequency of that term in the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b501c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:59:07.229677Z",
     "start_time": "2021-10-18T22:59:07.216681Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d03b771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:00:10.087467Z",
     "start_time": "2021-10-18T23:00:10.074094Z"
    }
   },
   "source": [
    "Convert the reuters corpus (at least one category) to its TF-IDF representation using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Beware of all the parameters of the methods! They might have significant impact on what you're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "977a2994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:51.654996Z",
     "start_time": "2021-10-25T13:01:51.634915Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Converting the corpus as full strings instead of words\n",
    "corpus_str = [' '.join(doc) for doc in corpus]\n",
    "tfidf_corpus = vectorizer.fit_transform(corpus_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa679b5f",
   "metadata": {},
   "source": [
    "With only three lines of code, you obtain TF-IDF embeddings for your $n$ documents, with all words scored each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73b24e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:53.490041Z",
     "start_time": "2021-10-25T13:01:53.482869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1134)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51649cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T11:06:45.792282Z",
     "start_time": "2021-10-25T11:06:45.789313Z"
    }
   },
   "source": [
    "You can access the vocabulary words by using the below method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df7507cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:54.342472Z",
     "start_time": "2021-10-25T13:01:54.332442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1960s', 'abnormal', 'abnormally', ..., 'zero', 'zimbabwe',\n",
       "       'zones'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b8648",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0b2a9",
   "metadata": {},
   "source": [
    "Once you have converted your corpus using the TF-IDF methodology, create a function identifying the most relevant comments given a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "647bfc73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:58.736821Z",
     "start_time": "2021-10-25T13:01:58.727546Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_corpus(corpus, search_query, topn=10):\n",
    "    \"\"\"Retrieve the top n documents matching a search query within a list of texts.\n",
    "    \"\"\"\n",
    "    corpus_str = [' '.join(doc) for doc in corpus]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_corpus = vectorizer.fit_transform(corpus_str)\n",
    "\n",
    "    query_vector = vectorizer.transform([search_query])[0]\n",
    "    \n",
    "    scores = [(corpus_str[i], cosine_similarity(query_vector, text_vector)[0][0]) for i, text_vector in enumerate(tfidf_corpus)]\n",
    "    \n",
    "    return pd.DataFrame(sorted(scores, reverse=True, key=lambda x: x[1])[:topn], columns=[\"document\", \"similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f761ccfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T13:01:59.547307Z",
     "start_time": "2021-10-25T13:01:59.523243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;START&gt; pakistani decision will hurt kenyan te...</td>\n",
       "      <td>0.197751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;START&gt; state to control pct of pakistan tea i...</td>\n",
       "      <td>0.143906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;START&gt; pakistan confirms kenya tea import inv...</td>\n",
       "      <td>0.102723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;START&gt; abnormal radiation found in soviet tea...</td>\n",
       "      <td>0.097015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;START&gt; indonesian tea cocoa exports seen up c...</td>\n",
       "      <td>0.089477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;START&gt; sri lankan tea workers launch one day ...</td>\n",
       "      <td>0.071925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;START&gt; vietnam to resettle on state farms in ...</td>\n",
       "      <td>0.035178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;START&gt; soviet paper details georgian flood da...</td>\n",
       "      <td>0.026356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;START&gt; india steps up countertrade deals indi...</td>\n",
       "      <td>0.021954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;START&gt; india relaxes rules for export promoti...</td>\n",
       "      <td>0.020039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  similarity\n",
       "0  <START> pakistani decision will hurt kenyan te...    0.197751\n",
       "1  <START> state to control pct of pakistan tea i...    0.143906\n",
       "2  <START> pakistan confirms kenya tea import inv...    0.102723\n",
       "3  <START> abnormal radiation found in soviet tea...    0.097015\n",
       "4  <START> indonesian tea cocoa exports seen up c...    0.089477\n",
       "5  <START> sri lankan tea workers launch one day ...    0.071925\n",
       "6  <START> vietnam to resettle on state farms in ...    0.035178\n",
       "7  <START> soviet paper details georgian flood da...    0.026356\n",
       "8  <START> india steps up countertrade deals indi...    0.021954\n",
       "9  <START> india relaxes rules for export promoti...    0.020039"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_corpus(corpus, search_query=\"tea regulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737bc75",
   "metadata": {},
   "source": [
    "Congrats! You have created your first text based search engine. Again, the results are not impressive here: you should pick a bigger dataset so that the embeddings are more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23a197",
   "metadata": {},
   "source": [
    "## Unsupervised Random Walk Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb91d27",
   "metadata": {},
   "source": [
    "This approach has been presented by Kawin Ethayarajh in 2018. The key idea behind this methodology is to take a weighted average of previously trained word embeddings and modify it with SVD (Singular Value Decomposition, a kind of generalization of the PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c36d8",
   "metadata": {},
   "source": [
    "By having a look at [this implementation](https://github.com/kawine/usif), try to compute the uSIF embeddings of our corpus and compare their properties to the TF-IDF / Averaged Word2Vec ones. \n",
    "\n",
    "Consider digging in the [related paper](https://aclanthology.org/W18-3012.pdf) if you want to know more about the methodology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course_ESIEA",
   "language": "python",
   "name": "course_esiea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
