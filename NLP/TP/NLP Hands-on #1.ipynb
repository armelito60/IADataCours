{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Hands-on # 1</center>\n",
    "<center><span style=\"font-weight: bold; font-size: 1.8rem;\">Representing words and sentences</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the algorithms existing out there are designed to handle numerical data, they are hardly applicable on raw texts. However, it is definitely possible to convert a text to a numerical representation.\n",
    "\n",
    "Ideal representations should handle **semantic**, **polysemy**, **irony** and lots of other specificities of texts. Along the decades, many text representations have been introduced to handle as many specificities as possible.\n",
    "\n",
    "In this hands-on, you will have to convert a given corpus of texts to various representations and highlight their pros / cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation of required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages listed below should be installed. Using a virtual environment is highly recommended but not mandatory -- that is just good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\logiciels\\anaconda\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.0 in c:\\logiciels\\anaconda\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.23 in c:\\logiciels\\anaconda\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\logiciels\\anaconda\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\logiciels\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:55:57.903290Z",
     "start_time": "2021-10-18T22:55:56.720039Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "# Uncomment the following line to download the reuters dataset\n",
    "# nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In NLP, we often add `<START>` and `<END>` tokens to represent the beginning and end of sentences, paragraphs or documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0 - Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters Corpus that we will use contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 categories.\n",
    "\n",
    "Before diving into word representations, let's explore it a little bit and simply preprocess its texts to make it more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to standardize all texts before converting anything to a numerical representations, since it will reduce the vocabulary size. Modify the following function to:\n",
    "\n",
    "* Add the `START_TOKEN` and `END_TOKEN` at the beginning and end of each document\n",
    "* Lowercase every words\n",
    "* Remove the punctuation from each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Armel\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:20:38.319875Z",
     "start_time": "2021-10-18T22:20:38.310517Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_corpus(category=\"tea\"):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\" \n",
    "    files = reuters.fileids(category)\n",
    "    for f in files:\n",
    "        current_text = list(reuters.words(f))\n",
    "        current_text.lower()\n",
    "        current_text = [START_TOKEN] + current_text + [END_TOKEN]\n",
    "        \n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:20:40.069771Z",
     "start_time": "2021-10-18T22:20:40.023236Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'SOVIET']\n"
     ]
    }
   ],
   "source": [
    "pp(corpus[:2], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our texts we can represent them using vectors, also called embeddings in this case.\n",
    "\n",
    "*Note: the preprocessing done here is basic. We will see in another hands-on different preprocessing steps, including some suitable for frequentist approaches.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word representation has its pros and cons: understanding them will help you in finding the best representation that suits your use case. \n",
    "\n",
    "As a result, you will have to implement / load and analyze the behaviour of word vectors coming from:\n",
    "\n",
    "* Dummy encoding\n",
    "* Co-occurence matrix encoding\n",
    "* Pretrained GloVe encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy encoding consist in encoding each individual word of our corpus into a vector filled with 0 expect at a specific position where the value is 1 (equivalent to the encoding of categorical variables).\n",
    "\n",
    "As discussed during the course, those embeddings are kind of pointless since they don't handle a single element of the ideal word representation beside being actual vectors. They are however a good starting point to play around with texts.\n",
    "\n",
    "---\n",
    "\n",
    "Define a function converting the words of a corpus to a set of dummy encoded vectors. Do not forget to sort your vocabulary before assigning the vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:14:38.935786Z",
     "start_time": "2021-10-18T23:14:38.919094Z"
    }
   },
   "outputs": [],
   "source": [
    "def dummy_encode(corpus):\n",
    "    \"\"\"One-hot encoding of a set of texts.\"\"\"\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_embeddings = dummy_encode(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still do not believe that this representation is pointless, try finding the most similar word to \"cat\" using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurence matrix encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section comes from Stanford's NLP hands-on*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A co-occurrence matrix counts how often things co-occur in some environment. Given some word  $w_i$  occurring in the document, we consider the context window surrounding  $w_i$ . Supposing our fixed window size is  $n$ , then this is the  $n$  preceding and  $n$  subsequent words in that document, i.e. words  $w_{i−n} … w_{i−1}$  and  $w_{i+1} … w{i+n}$ . We build a co-occurrence matrix  $M$ , which is a symmetric word-by-word matrix in which  $M_{ij}$  is the number of times  $w_j$  appears inside  $w_i$ 's window among all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Co-Occurrence with Fixed Window of n=1:**\n",
    "\n",
    "* Document 1: \"all that glitters is not gold\"\n",
    "\n",
    "* Document 2: \"all is well that ends well\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        * \t| <START> \t| all \t| that \t| glitters \t| is \t| not \t| gold \t| well \t| ends \t| <END> \t|\n",
    "|---------:\t|--------:\t|----:\t|-----:\t|---------:\t|---:\t|----:\t|-----:\t|-----:\t|-----:\t|------:\t|\n",
    "|  <START> \t|       0 \t|   2 \t|    0 \t|        0 \t|  0 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|      all \t|       2 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|     that \t|       0 \t|   1 \t|    0 \t|        1 \t|  0 \t|   0 \t|    0 \t|    1 \t|    1 \t|     0 \t|\n",
    "| glitters \t|       0 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    0 \t|     0 \t|\n",
    "|       is \t|       0 \t|   1 \t|    0 \t|        1 \t|  0 \t|   1 \t|    0 \t|    1 \t|    0 \t|     0 \t|\n",
    "|      not \t|       0 \t|   0 \t|    0 \t|        0 \t|  1 \t|   0 \t|    1 \t|    0 \t|    0 \t|     0 \t|\n",
    "|     gold \t|       0 \t|   0 \t|    0 \t|        0 \t|  0 \t|   1 \t|    0 \t|    0 \t|    0 \t|     1 \t|\n",
    "|     well \t|       0 \t|   0 \t|    1 \t|        0 \t|  1 \t|   0 \t|    0 \t|    0 \t|    1 \t|     1 \t|\n",
    "|     ends \t|       0 \t|   0 \t|    1 \t|        0 \t|  0 \t|   0 \t|    0 \t|    1 \t|    0 \t|     0 \t|\n",
    "|    <END> \t|       0 \t|   0 \t|    0 \t|        0 \t|  0 \t|   0 \t|    1 \t|    1 \t|    0 \t|     0 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run dimensionality reduction. In particular, we will run *SVD* (Singular Value Decomposition), which is a kind of generalized *PCA* (Principal Components Analysis) to select the top  k  principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of such vectors doesn't interterfere with the semantic relationship between words. Hence, *movie* will still be closer to *theater* than to *airplane*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify distinct words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will return a list of unique words of the corpus as well as its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:29:44.233392Z",
     "start_time": "2021-10-18T23:29:44.214566Z"
    }
   },
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the co-occurence matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a method that constructs a co-occurrence matrix for a certain window-size  $n$  (with a default of $4$), considering words  $n$  before and  $n$  after the word in the center of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:32:16.377062Z",
     "start_time": "2021-10-18T23:32:16.354704Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
    "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the dimensionality of the co-occurence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a method that performs dimensionality reduction on the matrix to produce $k$-dimensional embeddings. Use *SVD* to take the top $k$ components and produce a new matrix of $k$-dimensional embeddings.\n",
    "\n",
    "In our case, we will set $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:36:01.823050Z",
     "start_time": "2021-10-18T23:36:01.802519Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "    \n",
    "    \n",
    "        # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now have fix-sized vectors that represent each words of your corpus. Let's normalize our matrix to compare our vectors easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M_reduced_co_occurrence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b1663804843f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Rescale (normalize) the rows to make them each of unit-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mM_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM_reduced_co_occurrence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mM_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM_reduced_co_occurrence\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mM_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# broadcasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'M_reduced_co_occurrence' is not defined"
     ]
    }
   ],
   "source": [
    "# Rescale (normalize) the rows to make them each of unit-length\n",
    "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with vectors, we can easily measure the similarity between them using the dot product. Hence, given a specific word and its related word embedding, we can easily identify its most similar words contained in the corpus!\n",
    "\n",
    "*Note: the dot product between two vectors is bounded between -1 and 1 and the dot product of two identical vectors is equal to 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that given a word $w$ identify its most similar words in your embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, matrix, topn=10):\n",
    "    \"\"\"Return the words that have the closest embedding to the queried word.\"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return ranked_words[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec models are predictive by essence, since it is a neural network. However, this is not the sole method to learn geometrical encodings (vectors) of words from their co-occurrence information (how frequently they appear together in large text corpora).\n",
    "\n",
    "GloVe is a count-based model that learn their vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix. Does it remind you of something? Yes, that's exactly what you did above.\n",
    "\n",
    "Building models is time consuming. Hence, GloVe / Word2Vec models already trained on regular training sets (Wikipedia, News, etc.) are publicly shared to be reused easily.\n",
    "\n",
    "The below code will load a Glove model trained on wikipedia and allow us inspect easily the embeddings properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:47:19.997494Z",
     "start_time": "2021-10-18T22:47:19.315155Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:56:08.865842Z",
     "start_time": "2021-10-18T22:56:08.488793Z"
    }
   },
   "outputs": [],
   "source": [
    "# List all pretrained models available on gensim*\n",
    "pp(list(api.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:50:53.003715Z",
     "start_time": "2021-10-18T22:50:25.811223Z"
    }
   },
   "outputs": [],
   "source": [
    "model = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the most similar embeddings to those of a given set of words. \n",
    "Here, we retrieve the most similar words to fox, rabbit and cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:05:44.269302Z",
     "start_time": "2021-10-18T23:05:44.241616Z"
    }
   },
   "outputs": [],
   "source": [
    "pp(model.most_similar(positive=[\"fox\", \"rabbit\", \"cat\"]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform concept additions / soustractions. For instance, you can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:06:51.602919Z",
     "start_time": "2021-10-18T23:06:51.577069Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore those embeddings and comment on how they help to identify / handle:\n",
    "\n",
    "* Synonyms\n",
    "* Antonyms\n",
    "* Grammatical errors\n",
    "* Polysemy\n",
    "* Irony\n",
    "* Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where do you think those biases come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF is a methodology aiming at finding the most significative words in each document by comparing their in-document frequency to the overall frequency of that term in the whole corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T22:59:07.229677Z",
     "start_time": "2021-10-18T22:59:07.216681Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T23:00:10.087467Z",
     "start_time": "2021-10-18T23:00:10.074094Z"
    }
   },
   "source": [
    "Convert the reuters corpus (at least one category) to its TF-IDF representation using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Beware of all the parameters of the methods! They might have significant impact on what you're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have converted your corpus using the TF-IDF methodology, create a function identifying the most relevant comments given a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_corpus(corpus, search_query, topn=10):\n",
    "    \"\"\"Retrieve the top n documents matching a search query within a list of texts.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tfidf_corpus = TfidfVectorizer(...)\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return ranked_articles[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Random Walk Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has been presented by Kawin Ethayarajh in 2018. The key idea behind this methodology is to take a weighted average of previously trained word embeddings and modify it with SVD (Singular Value Decomposition, a kind of generalization of the PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By having a look at [this implementation](https://github.com/kawine/usif), try to compute the uSIF embeddings of our corpus and compare their properties to the TF-IDF / Averaged Word2Vec ones. \n",
    "\n",
    "Consider digging in the [related paper](https://aclanthology.org/W18-3012.pdf) if you want to know more about the methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
